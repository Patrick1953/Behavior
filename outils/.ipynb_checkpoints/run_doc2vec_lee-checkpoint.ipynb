{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2265"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lecture du fichier en fancais\n",
    "import smart_open\n",
    "\n",
    "def read_corpus(fname, tokens_only=True):\n",
    "    \n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            continue\n",
    "    return i\n",
    "\n",
    "fname = \"../data/texte_francais/texte.txt\"\n",
    "\n",
    "\n",
    "read_corpus(fname, tokens_only=True)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Doc2Vec Model\n",
    "=============\n",
    "\n",
    "Introduces Gensim's Doc2Vec model and demonstrates its use on the\n",
    "`Lee Corpus <https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`__.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec is a `core_concepts_model` that represents each\n",
    "`core_concepts_document` as a `core_concepts_vector`.  This\n",
    "tutorial introduces the model and demonstrates how to train and assess it.\n",
    "\n",
    "Here's a list of what we'll be doing:\n",
    "\n",
    "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
    "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
    "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
    "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
    "4. Assess the model\n",
    "5. Test the model on the test corpus\n",
    "\n",
    "Review: Bag-of-words\n",
    "--------------------\n",
    "\n",
    ".. Note:: Feel free to skip these review sections if you're already familiar with the models.\n",
    "\n",
    "You may be familiar with the `bag-of-words model\n",
    "<https://en.wikipedia.org/wiki/Bag-of-words_model>`_ from the\n",
    "`core_concepts_vector` section.\n",
    "This model transforms each document to a fixed-length vector of integers.\n",
    "For example, given the sentences:\n",
    "\n",
    "- ``John likes to watch movies. Mary likes movies too.``\n",
    "- ``John also likes to watch football games. Mary hates football.``\n",
    "\n",
    "The model outputs the vectors:\n",
    "\n",
    "- ``[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]``\n",
    "- ``[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]``\n",
    "\n",
    "Each vector has 10 elements, where each element counts the number of times a\n",
    "particular word occurred in the document.\n",
    "The order of elements is arbitrary.\n",
    "In the example above, the order of the elements corresponds to the words:\n",
    "``[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\", \"hates\"]``.\n",
    "\n",
    "Bag-of-words models are surprisingly effective, but have several weaknesses.\n",
    "\n",
    "First, they lose all information about word order: \"John likes Mary\" and\n",
    "\"Mary likes John\" correspond to identical vectors. There is a solution: bag\n",
    "of `n-grams <https://en.wikipedia.org/wiki/N-gram>`__\n",
    "models consider word phrases of length n to represent documents as\n",
    "fixed-length vectors to capture local word order but suffer from data\n",
    "sparsity and high dimensionality.\n",
    "\n",
    "Second, the model does not attempt to learn the meaning of the underlying\n",
    "words, and as a consequence, the distance between vectors doesn't always\n",
    "reflect the difference in meaning.  The ``Word2Vec`` model addresses this\n",
    "second problem.\n",
    "\n",
    "Review: ``Word2Vec`` Model\n",
    "--------------------------\n",
    "\n",
    "``Word2Vec`` is a more recent model that embeds words in a lower-dimensional\n",
    "vector space using a shallow neural network. The result is a set of\n",
    "word-vectors where vectors close together in vector space have similar\n",
    "meanings based on context, and word-vectors distant to each other have\n",
    "differing meanings. For example, ``strong`` and ``powerful`` would be close\n",
    "together and ``strong`` and ``Paris`` would be relatively far.\n",
    "\n",
    "Gensim's :py:class:`~gensim.models.word2vec.Word2Vec` class implements this model.\n",
    "\n",
    "With the ``Word2Vec`` model, we can calculate the vectors for each **word** in a document.\n",
    "But what if we want to calculate a vector for the **entire document**\\ ?\n",
    "We could average the vectors for each word in the document - while this is quick and crude, it can often be useful.\n",
    "However, there is a better way...\n",
    "\n",
    "Introducing: Paragraph Vector\n",
    "-----------------------------\n",
    "\n",
    ".. Important:: In Gensim, we refer to the Paragraph Vector model as ``Doc2Vec``.\n",
    "\n",
    "Le and Mikolov in 2014 introduced the `Doc2Vec algorithm <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`__,\n",
    "which usually outperforms such simple-averaging of ``Word2Vec`` vectors.\n",
    "\n",
    "The basic idea is: act as if a document has another floating word-like\n",
    "vector, which contributes to all training predictions, and is updated like\n",
    "other word-vectors, but we will call it a doc-vector. Gensim's\n",
    ":py:class:`~gensim.models.doc2vec.Doc2Vec` class implements this algorithm.\n",
    "\n",
    "There are two implementations:\n",
    "\n",
    "1. Paragraph Vector - Distributed Memory (PV-DM)\n",
    "2. Paragraph Vector - Distributed Bag of Words (PV-DBOW)\n",
    "\n",
    ".. Important::\n",
    "  Don't let the implementation details below scare you.\n",
    "  They're advanced material: if it's too much, then move on to the next section.\n",
    "\n",
    "PV-DM is analogous to Word2Vec CBOW. The doc-vectors are obtained by training\n",
    "a neural network on the synthetic task of predicting a center word based an\n",
    "average of both context word-vectors and the full document's doc-vector.\n",
    "\n",
    "PV-DBOW is analogous to Word2Vec SG. The doc-vectors are obtained by training\n",
    "a neural network on the synthetic task of predicting a target word just from\n",
    "the full document's doc-vector. (It is also common to combine this with\n",
    "skip-gram testing, using both the doc-vector and nearby word-vectors to\n",
    "predict a single target word, but only one at a time.)\n",
    "\n",
    "Prepare the Training and Test Data\n",
    "----------------------------------\n",
    "\n",
    "For this tutorial, we'll be training our model using the `Lee Background\n",
    "Corpus\n",
    "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n",
    "included in gensim. This corpus contains 314 documents selected from the\n",
    "Australian Broadcasting Corporation’s news mail service, which provides text\n",
    "e-mails of headline stories and covers a number of broad topics.\n",
    "\n",
    "And we'll test our model by eye using the much shorter `Lee Corpus\n",
    "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n",
    "which contains 50 documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "# Set file names for train and test data\n",
    "test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n",
    "lee_train_file = os.path.join(test_data_dir, 'lee_background.cor')\n",
    "lee_test_file = os.path.join(test_data_dir, 'lee.cor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Function to Read and Preprocess Text\n",
    "---------------------------------------------\n",
    "\n",
    "Below, we define a function to:\n",
    "\n",
    "- open the train/test file (with latin encoding)\n",
    "- read the file line-by-line\n",
    "- pre-process each line (tokenize text into individual words, remove punctuation, set to lowercase, etc)\n",
    "\n",
    "The file we're reading is a **corpus**.\n",
    "Each line of the file is a **document**.\n",
    "\n",
    ".. Important::\n",
    "  To train the model, we'll need to associate a tag/number with each document\n",
    "  of the training corpus. In our case, the tag is simply the zero-based line\n",
    "  number.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5194\n",
      "5194\n"
     ]
    }
   ],
   "source": [
    "import smart_open\n",
    "import gensim\n",
    "\n",
    "def read_corpus(fname1, fname2, tokens_only=False):\n",
    "    with smart_open.open(fname1, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "    j = i        \n",
    "    with smart_open.open(fname2, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "    print (i+j)       \n",
    "    return\n",
    "\n",
    "fname1 = \"../data/texte_francais/texte2.txt\"\n",
    "fname2 = \"../data/texte_francais/texte2.txt\"\n",
    "train_corpus = list(read_corpus(fname1, fname2))\n",
    "test_corpus = list(read_corpus(fname1, fname2, tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the training corpus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=[], tags=[0]), TaggedDocument(words=['pour', 'un', 'observateur', 'superficiel', 'la', 'vérité', 'scientifique', 'est', 'hors', 'des', 'atteintes', 'du', 'doute', 'la', 'logique', 'de', 'la', 'science', 'est', 'infaillible', 'et', 'si', 'les', 'savants', 'se', 'trompent', 'quelquefois', 'est', 'pour', 'en', 'avoir', 'méconnu', 'les', 'règles'], tags=[1])]\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the testing corpus looks like this:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['je', 'étais', 'pris', 'une', 'profonde', 'sympathie', 'pour', 'ce', 'grand', 'flemmard', 'de', 'gabelou', 'que', 'me', 'semblait', 'image', 'même', 'de', 'la', 'douane', 'non', 'pas', 'de', 'la', 'douane', 'tracassière', 'des', 'frontières', 'terriennes', 'mais', 'de', 'la', 'bonne', 'douane', 'flâneuse', 'et', 'contemplative', 'des', 'falaises', 'et', 'des', 'grèves'], ['son', 'nom', 'était', 'pascal', 'or', 'il', 'aurait', 'dû', 'appeler', 'baptiste', 'tant', 'il', 'apportait', 'de', 'douce', 'quiétude', 'accomplir', 'tous', 'les', 'actes', 'de', 'sa', 'vie'], ['et', 'était', 'plaisir', 'de', 'le', 'voir', 'les', 'mains', 'derrière', 'le', 'dos', 'traîner', 'lentement', 'ses', 'trois', 'heures', 'de', 'faction', 'sur', 'les', 'quais', 'de', 'préférence', 'ceux', 'où', 'ne', 'amarraient', 'que', 'des', 'barques', 'hors', 'usage', 'et', 'des', 'yachts', 'désarmés'], ['aussitôt', 'son', 'service', 'terminé', 'vite', 'pascal', 'abandonnait', 'son', 'pantalon', 'bleu', 'et', 'sa', 'tunique', 'verte', 'pour', 'enfiler', 'une', 'cotte', 'de', 'toile', 'et', 'une', 'longue', 'blouse', 'laquelle', 'des', 'coups', 'de', 'soleil', 'sans', 'nombre', 'et', 'des', 'averses', 'diluviennes', 'peut', 'être', 'même', 'antédiluviennes', 'avaient', 'donné', 'ce', 'ton', 'spécial', 'qu', 'on', 'ne', 'trouve', 'que', 'sur', 'le', 'dos', 'des', 'pêcheurs', 'la', 'ligne', 'car', 'pascal', 'pêchait', 'la', 'ligne', 'comme', 'feu', 'monseigneur', 'le', 'prince', 'de', 'ligne', 'lui', 'même']]\n"
     ]
    }
   ],
   "source": [
    "print(test_corpus[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the testing corpus is just a list of lists and does not contain\n",
    "any tags.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "Now, we'll instantiate a Doc2Vec model with a vector size with 50 dimensions and\n",
    "iterating over the training corpus 40 times. We set the minimum word count to\n",
    "2 in order to discard words with very few occurrences. (Without a variety of\n",
    "representative examples, retaining such infrequent words can often make a\n",
    "model worse!) Typical iteration counts in the published `Paragraph Vector paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`__\n",
    "results, using 10s-of-thousands to millions of docs, are 10-20. More\n",
    "iterations take more time and eventually reach a point of diminishing\n",
    "returns.\n",
    "\n",
    "However, this is a very very small dataset (300 documents) with shortish\n",
    "documents (a few hundred words). Adding training passes can sometimes help\n",
    "with such small datasets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-05 02:28:26,532 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50,\n",
    "                                      window = 88,\n",
    "                                      min_count=2,\n",
    "                                      worker = 10,\n",
    "                                      epochs= 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-05 02:28:28,595 : INFO : collecting all words and their counts\n",
      "2021-03-05 02:28:28,596 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2021-03-05 02:28:28,615 : INFO : collected 5391 word types and 2598 unique tags from a corpus of 5196 examples and 119156 words\n",
      "2021-03-05 02:28:28,616 : INFO : Loading a fresh vocabulary\n",
      "2021-03-05 02:28:28,622 : INFO : effective_min_count=2 retains 5391 unique words (100% of original 5391, drops 0)\n",
      "2021-03-05 02:28:28,622 : INFO : effective_min_count=2 leaves 119156 word corpus (100% of original 119156, drops 0)\n",
      "2021-03-05 02:28:28,632 : INFO : deleting the raw counts dictionary of 5391 items\n",
      "2021-03-05 02:28:28,632 : INFO : sample=0.001 downsamples 50 most-common words\n",
      "2021-03-05 02:28:28,632 : INFO : downsampling leaves estimated 85915 word corpus (72.1% of prior 119156)\n",
      "2021-03-05 02:28:28,638 : INFO : estimated required memory for 5391 words and 50 dimensions: 5371500 bytes\n",
      "2021-03-05 02:28:28,638 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the vocabulary is a list (accessible via\n",
    "``model.wv.index_to_key``) of all of the unique words extracted from the training corpus.\n",
    "Additional attributes for each word are available using the ``model.wv.get_vecattr()`` method,\n",
    "For example, to see how many times ``penalty`` appeared in the training corpus:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Word 'penalty' appeared {model.wv.get_vecattr('penalty', 'count')} times in the training corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, train the model on the corpus.\n",
    "If optimized Gensim (with BLAS library) is being used, this should take no more than 3 seconds.\n",
    "If the BLAS library is not being used, this should take no more than 2\n",
    "minutes, so use optimized Gensim with BLAS if you value your time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-05 02:28:34,486 : INFO : training model with 3 workers on 5391 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=88\n",
      "2021-03-05 02:28:34,691 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:34,693 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:34,702 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:34,703 : INFO : EPOCH - 1 : training on 119156 raw words (91183 effective words) took 0.2s, 427110 effective words/s\n",
      "2021-03-05 02:28:34,916 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:34,918 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:34,920 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:34,921 : INFO : EPOCH - 2 : training on 119156 raw words (91213 effective words) took 0.2s, 428323 effective words/s\n",
      "2021-03-05 02:28:35,147 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:35,151 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:35,159 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:35,159 : INFO : EPOCH - 3 : training on 119156 raw words (91034 effective words) took 0.2s, 387737 effective words/s\n",
      "2021-03-05 02:28:35,356 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:35,360 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:35,370 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:35,371 : INFO : EPOCH - 4 : training on 119156 raw words (91172 effective words) took 0.2s, 441686 effective words/s\n",
      "2021-03-05 02:28:35,572 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:35,588 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:35,590 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:35,590 : INFO : EPOCH - 5 : training on 119156 raw words (91157 effective words) took 0.2s, 425834 effective words/s\n",
      "2021-03-05 02:28:35,800 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:35,803 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:35,807 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:35,807 : INFO : EPOCH - 6 : training on 119156 raw words (91152 effective words) took 0.2s, 426502 effective words/s\n",
      "2021-03-05 02:28:36,037 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:36,037 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:36,040 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:36,040 : INFO : EPOCH - 7 : training on 119156 raw words (91206 effective words) took 0.2s, 400374 effective words/s\n",
      "2021-03-05 02:28:36,266 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:36,267 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:36,268 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:36,268 : INFO : EPOCH - 8 : training on 119156 raw words (91037 effective words) took 0.2s, 405642 effective words/s\n",
      "2021-03-05 02:28:36,478 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:36,480 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:36,490 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:36,490 : INFO : EPOCH - 9 : training on 119156 raw words (91163 effective words) took 0.2s, 418214 effective words/s\n",
      "2021-03-05 02:28:36,700 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:36,714 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:36,719 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:36,720 : INFO : EPOCH - 10 : training on 119156 raw words (91013 effective words) took 0.2s, 405557 effective words/s\n",
      "2021-03-05 02:28:36,927 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:36,940 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:36,941 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:36,942 : INFO : EPOCH - 11 : training on 119156 raw words (91220 effective words) took 0.2s, 416575 effective words/s\n",
      "2021-03-05 02:28:37,146 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:37,149 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:37,158 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:37,159 : INFO : EPOCH - 12 : training on 119156 raw words (91022 effective words) took 0.2s, 426936 effective words/s\n",
      "2021-03-05 02:28:37,341 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:37,343 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:37,347 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:37,347 : INFO : EPOCH - 13 : training on 119156 raw words (91169 effective words) took 0.2s, 494517 effective words/s\n",
      "2021-03-05 02:28:37,568 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:37,575 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:37,578 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:37,578 : INFO : EPOCH - 14 : training on 119156 raw words (91026 effective words) took 0.2s, 399403 effective words/s\n",
      "2021-03-05 02:28:37,797 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:37,801 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:37,805 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:37,805 : INFO : EPOCH - 15 : training on 119156 raw words (91185 effective words) took 0.2s, 411142 effective words/s\n",
      "2021-03-05 02:28:38,025 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:38,033 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:38,037 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:38,038 : INFO : EPOCH - 16 : training on 119156 raw words (91188 effective words) took 0.2s, 401334 effective words/s\n",
      "2021-03-05 02:28:38,251 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:38,259 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:38,265 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:38,266 : INFO : EPOCH - 17 : training on 119156 raw words (90975 effective words) took 0.2s, 409185 effective words/s\n",
      "2021-03-05 02:28:38,478 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:38,485 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:38,495 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:38,495 : INFO : EPOCH - 18 : training on 119156 raw words (91234 effective words) took 0.2s, 402666 effective words/s\n",
      "2021-03-05 02:28:38,654 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:38,661 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:38,665 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:38,666 : INFO : EPOCH - 19 : training on 119156 raw words (91006 effective words) took 0.2s, 550318 effective words/s\n",
      "2021-03-05 02:28:38,868 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:38,870 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:38,879 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:38,879 : INFO : EPOCH - 20 : training on 119156 raw words (91204 effective words) took 0.2s, 432949 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-05 02:28:39,106 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:39,113 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:39,114 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:39,115 : INFO : EPOCH - 21 : training on 119156 raw words (90942 effective words) took 0.2s, 394886 effective words/s\n",
      "2021-03-05 02:28:39,313 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:39,331 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:39,334 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:39,335 : INFO : EPOCH - 22 : training on 119156 raw words (91303 effective words) took 0.2s, 423735 effective words/s\n",
      "2021-03-05 02:28:39,547 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:39,554 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:39,558 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:39,559 : INFO : EPOCH - 23 : training on 119156 raw words (91366 effective words) took 0.2s, 414114 effective words/s\n",
      "2021-03-05 02:28:39,776 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:39,782 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:39,782 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:39,782 : INFO : EPOCH - 24 : training on 119156 raw words (90913 effective words) took 0.2s, 414681 effective words/s\n",
      "2021-03-05 02:28:39,991 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:40,002 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:40,003 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:40,003 : INFO : EPOCH - 25 : training on 119156 raw words (91232 effective words) took 0.2s, 423222 effective words/s\n",
      "2021-03-05 02:28:40,229 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:40,231 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:40,235 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:40,236 : INFO : EPOCH - 26 : training on 119156 raw words (91215 effective words) took 0.2s, 402251 effective words/s\n",
      "2021-03-05 02:28:40,455 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:40,456 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:40,461 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:40,461 : INFO : EPOCH - 27 : training on 119156 raw words (91021 effective words) took 0.2s, 412152 effective words/s\n",
      "2021-03-05 02:28:40,678 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:40,679 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:40,686 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:40,686 : INFO : EPOCH - 28 : training on 119156 raw words (91048 effective words) took 0.2s, 411679 effective words/s\n",
      "2021-03-05 02:28:40,897 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:40,902 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:40,914 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:40,914 : INFO : EPOCH - 29 : training on 119156 raw words (91100 effective words) took 0.2s, 404316 effective words/s\n",
      "2021-03-05 02:28:41,133 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:41,135 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:41,140 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:41,140 : INFO : EPOCH - 30 : training on 119156 raw words (90929 effective words) took 0.2s, 407923 effective words/s\n",
      "2021-03-05 02:28:41,341 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:41,355 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:41,357 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:41,358 : INFO : EPOCH - 31 : training on 119156 raw words (91263 effective words) took 0.2s, 428003 effective words/s\n",
      "2021-03-05 02:28:41,569 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:41,578 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:41,580 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:41,580 : INFO : EPOCH - 32 : training on 119156 raw words (90969 effective words) took 0.2s, 414315 effective words/s\n",
      "2021-03-05 02:28:41,788 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:41,799 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:41,801 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:41,801 : INFO : EPOCH - 33 : training on 119156 raw words (91090 effective words) took 0.2s, 418062 effective words/s\n",
      "2021-03-05 02:28:42,008 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:42,019 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:42,020 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:42,021 : INFO : EPOCH - 34 : training on 119156 raw words (91237 effective words) took 0.2s, 421113 effective words/s\n",
      "2021-03-05 02:28:42,221 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:42,230 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:42,236 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:42,237 : INFO : EPOCH - 35 : training on 119156 raw words (91104 effective words) took 0.2s, 428499 effective words/s\n",
      "2021-03-05 02:28:42,425 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:42,431 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:42,434 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:42,434 : INFO : EPOCH - 36 : training on 119156 raw words (91284 effective words) took 0.2s, 472433 effective words/s\n",
      "2021-03-05 02:28:42,646 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:42,651 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:42,658 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:42,658 : INFO : EPOCH - 37 : training on 119156 raw words (91135 effective words) took 0.2s, 415440 effective words/s\n",
      "2021-03-05 02:28:42,871 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:42,890 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:42,891 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:42,892 : INFO : EPOCH - 38 : training on 119156 raw words (91076 effective words) took 0.2s, 398040 effective words/s\n",
      "2021-03-05 02:28:43,089 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:43,104 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:43,109 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:43,109 : INFO : EPOCH - 39 : training on 119156 raw words (91270 effective words) took 0.2s, 426086 effective words/s\n",
      "2021-03-05 02:28:43,309 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:43,310 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:43,323 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:43,323 : INFO : EPOCH - 40 : training on 119156 raw words (91159 effective words) took 0.2s, 436313 effective words/s\n",
      "2021-03-05 02:28:43,511 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-05 02:28:43,512 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:43,519 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:43,519 : INFO : EPOCH - 41 : training on 119156 raw words (91320 effective words) took 0.2s, 478334 effective words/s\n",
      "2021-03-05 02:28:43,680 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:43,690 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:43,692 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:43,693 : INFO : EPOCH - 42 : training on 119156 raw words (91134 effective words) took 0.2s, 538977 effective words/s\n",
      "2021-03-05 02:28:43,900 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:43,904 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:43,910 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:43,911 : INFO : EPOCH - 43 : training on 119156 raw words (91230 effective words) took 0.2s, 424451 effective words/s\n",
      "2021-03-05 02:28:44,121 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:44,123 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:44,131 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:44,131 : INFO : EPOCH - 44 : training on 119156 raw words (91105 effective words) took 0.2s, 418979 effective words/s\n",
      "2021-03-05 02:28:44,305 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:44,306 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:44,310 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:44,310 : INFO : EPOCH - 45 : training on 119156 raw words (91158 effective words) took 0.2s, 526072 effective words/s\n",
      "2021-03-05 02:28:44,485 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:44,487 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:44,489 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:44,490 : INFO : EPOCH - 46 : training on 119156 raw words (91264 effective words) took 0.2s, 523426 effective words/s\n",
      "2021-03-05 02:28:44,675 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:44,682 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:44,688 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:44,689 : INFO : EPOCH - 47 : training on 119156 raw words (91114 effective words) took 0.2s, 468650 effective words/s\n",
      "2021-03-05 02:28:44,870 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:44,872 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:44,878 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:44,878 : INFO : EPOCH - 48 : training on 119156 raw words (91263 effective words) took 0.2s, 490501 effective words/s\n",
      "2021-03-05 02:28:45,117 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:45,120 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:45,128 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:45,129 : INFO : EPOCH - 49 : training on 119156 raw words (91118 effective words) took 0.2s, 368542 effective words/s\n",
      "2021-03-05 02:28:45,330 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:45,331 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:45,338 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:45,339 : INFO : EPOCH - 50 : training on 119156 raw words (91173 effective words) took 0.2s, 446822 effective words/s\n",
      "2021-03-05 02:28:45,532 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:45,536 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:45,541 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:45,541 : INFO : EPOCH - 51 : training on 119156 raw words (90862 effective words) took 0.2s, 455465 effective words/s\n",
      "2021-03-05 02:28:45,734 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:45,737 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:45,742 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:45,742 : INFO : EPOCH - 52 : training on 119156 raw words (90984 effective words) took 0.2s, 464568 effective words/s\n",
      "2021-03-05 02:28:45,945 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:45,951 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:45,956 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:45,956 : INFO : EPOCH - 53 : training on 119156 raw words (91212 effective words) took 0.2s, 435523 effective words/s\n",
      "2021-03-05 02:28:46,160 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:46,162 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:46,169 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:46,170 : INFO : EPOCH - 54 : training on 119156 raw words (91285 effective words) took 0.2s, 433753 effective words/s\n",
      "2021-03-05 02:28:46,374 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:46,383 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:46,388 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:46,389 : INFO : EPOCH - 55 : training on 119156 raw words (91057 effective words) took 0.2s, 422343 effective words/s\n",
      "2021-03-05 02:28:46,601 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:46,613 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:46,617 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:46,617 : INFO : EPOCH - 56 : training on 119156 raw words (91261 effective words) took 0.2s, 408047 effective words/s\n",
      "2021-03-05 02:28:46,816 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:46,820 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:46,821 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:46,822 : INFO : EPOCH - 57 : training on 119156 raw words (91040 effective words) took 0.2s, 457001 effective words/s\n",
      "2021-03-05 02:28:47,021 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:47,022 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:47,036 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:47,036 : INFO : EPOCH - 58 : training on 119156 raw words (91058 effective words) took 0.2s, 429534 effective words/s\n",
      "2021-03-05 02:28:47,233 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:47,239 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:47,241 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:47,241 : INFO : EPOCH - 59 : training on 119156 raw words (90984 effective words) took 0.2s, 450230 effective words/s\n",
      "2021-03-05 02:28:47,407 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:47,409 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:47,413 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:47,413 : INFO : EPOCH - 60 : training on 119156 raw words (90943 effective words) took 0.2s, 538626 effective words/s\n",
      "2021-03-05 02:28:47,604 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:47,614 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-05 02:28:47,618 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:47,618 : INFO : EPOCH - 61 : training on 119156 raw words (91105 effective words) took 0.2s, 454367 effective words/s\n",
      "2021-03-05 02:28:47,824 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:47,829 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:47,839 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:47,839 : INFO : EPOCH - 62 : training on 119156 raw words (91287 effective words) took 0.2s, 422216 effective words/s\n",
      "2021-03-05 02:28:48,057 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:48,058 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:48,064 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:48,064 : INFO : EPOCH - 63 : training on 119156 raw words (90982 effective words) took 0.2s, 409961 effective words/s\n",
      "2021-03-05 02:28:48,270 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:48,276 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:48,277 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:48,278 : INFO : EPOCH - 64 : training on 119156 raw words (91045 effective words) took 0.2s, 435613 effective words/s\n",
      "2021-03-05 02:28:48,456 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:48,468 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:48,474 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:48,474 : INFO : EPOCH - 65 : training on 119156 raw words (91144 effective words) took 0.2s, 470551 effective words/s\n",
      "2021-03-05 02:28:48,655 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:48,662 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:48,668 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:48,669 : INFO : EPOCH - 66 : training on 119156 raw words (91010 effective words) took 0.2s, 475608 effective words/s\n",
      "2021-03-05 02:28:48,884 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:48,893 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:48,898 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:48,899 : INFO : EPOCH - 67 : training on 119156 raw words (91015 effective words) took 0.2s, 403650 effective words/s\n",
      "2021-03-05 02:28:49,125 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:49,129 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:49,132 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:49,132 : INFO : EPOCH - 68 : training on 119156 raw words (91275 effective words) took 0.2s, 400037 effective words/s\n",
      "2021-03-05 02:28:49,333 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:49,337 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:49,339 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:49,339 : INFO : EPOCH - 69 : training on 119156 raw words (91115 effective words) took 0.2s, 446808 effective words/s\n",
      "2021-03-05 02:28:49,522 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:49,526 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:49,532 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:49,533 : INFO : EPOCH - 70 : training on 119156 raw words (91013 effective words) took 0.2s, 482776 effective words/s\n",
      "2021-03-05 02:28:49,737 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:49,739 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:49,748 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:49,748 : INFO : EPOCH - 71 : training on 119156 raw words (90979 effective words) took 0.2s, 431319 effective words/s\n",
      "2021-03-05 02:28:49,966 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:49,966 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:49,974 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:49,974 : INFO : EPOCH - 72 : training on 119156 raw words (91134 effective words) took 0.2s, 409393 effective words/s\n",
      "2021-03-05 02:28:50,189 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:50,196 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:50,197 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:50,197 : INFO : EPOCH - 73 : training on 119156 raw words (91264 effective words) took 0.2s, 418081 effective words/s\n",
      "2021-03-05 02:28:50,403 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:50,409 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:50,412 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:50,412 : INFO : EPOCH - 74 : training on 119156 raw words (91113 effective words) took 0.2s, 431117 effective words/s\n",
      "2021-03-05 02:28:50,590 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:50,598 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:50,601 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:50,601 : INFO : EPOCH - 75 : training on 119156 raw words (91209 effective words) took 0.2s, 494437 effective words/s\n",
      "2021-03-05 02:28:50,808 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:50,810 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:50,814 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:50,814 : INFO : EPOCH - 76 : training on 119156 raw words (91058 effective words) took 0.2s, 434689 effective words/s\n",
      "2021-03-05 02:28:50,996 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:51,001 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:51,009 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:51,009 : INFO : EPOCH - 77 : training on 119156 raw words (91157 effective words) took 0.2s, 479174 effective words/s\n",
      "2021-03-05 02:28:51,219 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:51,232 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:51,234 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:51,234 : INFO : EPOCH - 78 : training on 119156 raw words (91195 effective words) took 0.2s, 411137 effective words/s\n",
      "2021-03-05 02:28:51,436 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:51,440 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:51,447 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:51,447 : INFO : EPOCH - 79 : training on 119156 raw words (91059 effective words) took 0.2s, 437492 effective words/s\n",
      "2021-03-05 02:28:51,673 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:51,674 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:51,680 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:51,680 : INFO : EPOCH - 80 : training on 119156 raw words (91251 effective words) took 0.2s, 400288 effective words/s\n",
      "2021-03-05 02:28:51,874 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:51,884 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:51,891 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-05 02:28:51,891 : INFO : EPOCH - 81 : training on 119156 raw words (91025 effective words) took 0.2s, 442225 effective words/s\n",
      "2021-03-05 02:28:52,090 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:52,093 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:52,104 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:52,104 : INFO : EPOCH - 82 : training on 119156 raw words (91108 effective words) took 0.2s, 438788 effective words/s\n",
      "2021-03-05 02:28:52,318 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:52,321 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:52,324 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:52,324 : INFO : EPOCH - 83 : training on 119156 raw words (91302 effective words) took 0.2s, 420907 effective words/s\n",
      "2021-03-05 02:28:52,507 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:52,513 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:52,515 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:52,516 : INFO : EPOCH - 84 : training on 119156 raw words (91486 effective words) took 0.2s, 486963 effective words/s\n",
      "2021-03-05 02:28:52,761 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:52,762 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:52,763 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:52,763 : INFO : EPOCH - 85 : training on 119156 raw words (91067 effective words) took 0.2s, 375332 effective words/s\n",
      "2021-03-05 02:28:52,980 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:52,993 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:53,001 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:53,002 : INFO : EPOCH - 86 : training on 119156 raw words (91112 effective words) took 0.2s, 389146 effective words/s\n",
      "2021-03-05 02:28:53,201 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:53,212 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:53,216 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:53,216 : INFO : EPOCH - 87 : training on 119156 raw words (90944 effective words) took 0.2s, 432828 effective words/s\n",
      "2021-03-05 02:28:53,401 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:53,416 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:53,417 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:53,417 : INFO : EPOCH - 88 : training on 119156 raw words (90870 effective words) took 0.2s, 464609 effective words/s\n",
      "2021-03-05 02:28:53,615 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:53,618 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:53,630 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:53,630 : INFO : EPOCH - 89 : training on 119156 raw words (90918 effective words) took 0.2s, 437202 effective words/s\n",
      "2021-03-05 02:28:53,854 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:53,860 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:53,862 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:53,863 : INFO : EPOCH - 90 : training on 119156 raw words (91292 effective words) took 0.2s, 401363 effective words/s\n",
      "2021-03-05 02:28:54,099 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:54,103 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:54,110 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:54,110 : INFO : EPOCH - 91 : training on 119156 raw words (90910 effective words) took 0.2s, 374123 effective words/s\n",
      "2021-03-05 02:28:54,302 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:54,303 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:54,310 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:54,310 : INFO : EPOCH - 92 : training on 119156 raw words (91079 effective words) took 0.2s, 462283 effective words/s\n",
      "2021-03-05 02:28:54,494 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:54,495 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:54,503 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:54,504 : INFO : EPOCH - 93 : training on 119156 raw words (91089 effective words) took 0.2s, 478839 effective words/s\n",
      "2021-03-05 02:28:54,666 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:54,669 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:54,671 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:54,672 : INFO : EPOCH - 94 : training on 119156 raw words (91143 effective words) took 0.2s, 551396 effective words/s\n",
      "2021-03-05 02:28:54,875 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:54,876 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:54,889 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:54,889 : INFO : EPOCH - 95 : training on 119156 raw words (90895 effective words) took 0.2s, 426222 effective words/s\n",
      "2021-03-05 02:28:55,090 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:55,096 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:55,099 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:55,099 : INFO : EPOCH - 96 : training on 119156 raw words (91145 effective words) took 0.2s, 444516 effective words/s\n",
      "2021-03-05 02:28:55,294 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:55,306 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:55,319 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:55,319 : INFO : EPOCH - 97 : training on 119156 raw words (91014 effective words) took 0.2s, 417784 effective words/s\n",
      "2021-03-05 02:28:55,527 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:55,537 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:55,539 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:55,539 : INFO : EPOCH - 98 : training on 119156 raw words (91038 effective words) took 0.2s, 423684 effective words/s\n",
      "2021-03-05 02:28:55,745 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:55,750 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:55,758 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:55,759 : INFO : EPOCH - 99 : training on 119156 raw words (91238 effective words) took 0.2s, 421461 effective words/s\n",
      "2021-03-05 02:28:55,986 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-05 02:28:55,987 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-05 02:28:55,997 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-05 02:28:55,998 : INFO : EPOCH - 100 : training on 119156 raw words (91127 effective words) took 0.2s, 385765 effective words/s\n",
      "2021-03-05 02:28:55,998 : INFO : training on a 11915600 raw words (9111992 effective words) took 21.5s, 423586 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the trained model to infer a vector for any piece of text\n",
    "by passing a list of words to the ``model.infer_vector`` function. This\n",
    "vector can then be compared with other vectors via cosine similarity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2056525 , -1.1725636 , -1.0447414 ,  1.0862755 , -1.70868   ,\n",
       "       -0.43567753, -0.1041388 , -0.30113143,  1.0674226 ,  0.6642615 ,\n",
       "        1.1667893 ,  0.27039403, -0.86941516, -0.6690931 , -0.46370542,\n",
       "        0.45434046, -0.13733366, -0.5943513 , -0.7166033 ,  0.35626587,\n",
       "       -0.5366945 , -0.3315349 , -0.10042678,  0.02881689,  0.8042875 ,\n",
       "        0.16042224, -2.2917142 , -0.9070515 ,  0.02983554, -0.5608142 ,\n",
       "       -0.19678785, -0.67522436, -0.36935374,  1.0539546 ,  0.16760218,\n",
       "       -0.9404603 ,  0.07326629, -0.9392241 , -1.7637426 ,  0.672086  ,\n",
       "        0.0098313 ,  0.75873876,  0.13340439, -0.388217  , -0.2828258 ,\n",
       "        0.09788162, -0.07933856, -1.185037  , -0.9618502 , -0.1865974 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_base = model.infer_vector(['je', 'pense', 'donc', 'je', 'suis', 'fatigué'])\n",
    "vector_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exemple = ['je', 'étais', 'pris', 'une', 'profonde', 'sympathie', 'pour', 'ce', 'grand', 'flemmard']\n",
    "#vector = model.infer_vector(exemple)\n",
    "exemple1 =['je', 'pense', 'donc', 'je', 'suis', 'fatigué']\n",
    "vecteur_next = model.infer_vector(exemple1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.0166552 , -1.0722448 , -0.8444709 ,  0.9225949 , -1.6093898 ,\n",
       "       -0.46209168, -0.14841424, -0.18578885,  1.0833005 ,  0.73281205,\n",
       "        1.2446328 ,  0.12015898, -0.7589468 , -0.66280687, -0.33211148,\n",
       "        0.23918507, -0.06910549, -0.64137757, -0.54724634,  0.14343965,\n",
       "       -0.44265214, -0.22432826, -0.19395755,  0.03700214,  0.7102029 ,\n",
       "        0.16268279, -2.304147  , -0.7527979 ,  0.02984409, -0.4222658 ,\n",
       "       -0.14770392, -0.5794857 , -0.51995766,  1.0778834 ,  0.05551802,\n",
       "       -0.9531923 ,  0.14819793, -0.8815619 , -1.6935819 ,  0.76814085,\n",
       "        0.09103793,  0.5447187 ,  0.28829896, -0.14062074, -0.28858274,\n",
       "        0.03472719,  0.00839908, -1.2342616 , -0.9696124 , -0.17815289],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecteur_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy.spacial'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-035926f33a5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcosine_similarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvecteur_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy.spacial'"
     ]
    }
   ],
   "source": [
    "import scipy.spacial\n",
    "\n",
    "cosine_similarity = 1 - spatial.distance.cosine(vector_base, Y = vecteur_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ``infer_vector()`` does *not* take a string, but rather a list of\n",
    "string tokens, which should have already been tokenized the same way as the\n",
    "``words`` property of original training document objects.\n",
    "\n",
    "Also note that because the underlying training/inference algorithms are an\n",
    "iterative approximation problem that makes use of internal randomization,\n",
    "repeated inferences of the same text will return slightly different vectors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing the Model\n",
    "-------------------\n",
    "\n",
    "To assess our new model, we'll first infer new vectors for each document of\n",
    "the training corpus, compare the inferred vectors with the training corpus,\n",
    "and then returning the rank of the document based on self-similarity.\n",
    "Basically, we're pretending as if the training corpus is some new unseen data\n",
    "and then seeing how they compare with the trained model. The expectation is\n",
    "that we've likely overfit our model (i.e., all of the ranks will be less than\n",
    "2) and so we should be able to find similar documents very easily.\n",
    "Additionally, we'll keep track of the second ranks for a comparison of less\n",
    "similar documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Doc2Vec' object has no attribute 'dv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4e272c2b6023>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minferred_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minferred_vector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdocid\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdocid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msims\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mranks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Doc2Vec' object has no attribute 'dv'"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how each document ranks with respect to the training corpus\n",
    "\n",
    "NB. Results vary between runs due to random seeding and very small corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, greater than 95% of the inferred documents are found to be most\n",
    "similar to itself and about 5% of the time it is mistakenly most similar to\n",
    "another document. Checking the inferred-vector against a\n",
    "training-vector is a sort of 'sanity check' as to whether the model is\n",
    "behaving in a usefully consistent manner, though not a real 'accuracy' value.\n",
    "\n",
    "This is great and not entirely surprising. We can take a look at an example:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that the most similar document (usually the same text) is has a\n",
    "similarity score approaching 1.0. However, the similarity score for the\n",
    "second-ranked documents should be significantly lower (assuming the documents\n",
    "are in fact different) and the reasoning becomes obvious when we examine the\n",
    "text itself.\n",
    "\n",
    "We can run the next cell repeatedly to see a sampling other target-document\n",
    "comparisons.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "import random\n",
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Model\n",
    "-----------------\n",
    "\n",
    "Using the same approach above, we'll infer the vector for a randomly chosen\n",
    "test document, and compare the document to our model by eye.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "----------\n",
    "\n",
    "Let's review what we've seen in this tutorial:\n",
    "\n",
    "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
    "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
    "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
    "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
    "4. Assess the model\n",
    "5. Test the model on the test corpus\n",
    "\n",
    "That's it! Doc2Vec is a great way to explore relationships between documents.\n",
    "\n",
    "Additional Resources\n",
    "--------------------\n",
    "\n",
    "If you'd like to know more about the subject matter of this tutorial, check out the links below.\n",
    "\n",
    "* `Word2Vec Paper <https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf>`_\n",
    "* `Doc2Vec Paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`_\n",
    "* `Dr. Michael D. Lee's Website <http://faculty.sites.uci.edu/mdlee>`_\n",
    "* `Lee Corpus <http://faculty.sites.uci.edu/mdlee/similarity-data/>`__\n",
    "* `IMDB Doc2Vec Tutorial <doc2vec-IMDB.ipynb>`_\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
