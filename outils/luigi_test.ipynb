{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version\n",
      "3.9.1 (default, Dec 11 2020, 14:32:07) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version\")\n",
    "print (sys.version)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "passer des chaines pas des listes pour la liste des ID\n",
    "pour des grosses faire json et compresion....vitesse\n",
    "\n",
    "/etc/luigi/luigi.cfg # fichier config lu par luigi Ã  chaque demarrage\n",
    "a modifier pour eviter les affichage des traces de luigi\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/parameter.py:279: UserWarning: Parameter \"x\" with value \"0\" is not of type string.\n",
      "  warnings.warn('Parameter \"{}\" with value \"{}\" is not of type string.'.format(param_name, param_value))\n",
      "/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/worker.py:401: UserWarning: Task MyTask1(x=0) without outputs has no custom complete() method\n",
      "  is_complete = task.complete()\n",
      "WARNING: Failed connecting to remote scheduler 'http://localhost:8082'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 169, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/util/connection.py\", line 96, in create_connection\n",
      "    raise err\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/util/connection.py\", line 86, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 699, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 394, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 234, in request\n",
      "    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1255, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1301, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1250, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1010, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 950, in send\n",
      "    self.connect()\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 200, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 181, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7efdb7f67fd0>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 755, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/util/retry.py\", line 573, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8082): Max retries exceeded with url: /api/add_task (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efdb7f67fd0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/rpc.py\", line 163, in _fetch\n",
      "    response = self._fetcher.fetch(full_url, body, self._connect_timeout)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/rpc.py\", line 116, in fetch\n",
      "    resp = self.session.post(full_url, data=body, timeout=timeout)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/sessions.py\", line 590, in post\n",
      "    return self.request('POST', url, data=data, json=json, **kwargs)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8082): Max retries exceeded with url: /api/add_task (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efdb7f67fd0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "WARNING: Failed connecting to remote scheduler 'http://localhost:8082'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 169, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/util/connection.py\", line 96, in create_connection\n",
      "    raise err\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/util/connection.py\", line 86, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 699, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 394, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 234, in request\n",
      "    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1255, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1301, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1250, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1010, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 950, in send\n",
      "    self.connect()\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 200, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 181, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7efdcc24fd30>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 755, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/util/retry.py\", line 573, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8082): Max retries exceeded with url: /api/ping (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efdcc24fd30>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/rpc.py\", line 163, in _fetch\n",
      "    response = self._fetcher.fetch(full_url, body, self._connect_timeout)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/rpc.py\", line 116, in fetch\n",
      "    resp = self.session.post(full_url, data=body, timeout=timeout)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/sessions.py\", line 590, in post\n",
      "    return self.request('POST', url, data=data, json=json, **kwargs)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8082): Max retries exceeded with url: /api/ping (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efdcc24fd30>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed connecting to remote scheduler 'http://localhost:8082'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 169, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/util/connection.py\", line 96, in create_connection\n",
      "    raise err\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/util/connection.py\", line 86, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 699, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 394, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 234, in request\n",
      "    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1255, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1301, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1250, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1010, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 950, in send\n",
      "    self.connect()\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 200, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 181, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7efdcc24fb50>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 755, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/util/retry.py\", line 573, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8082): Max retries exceeded with url: /api/add_task (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efdcc24fb50>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/rpc.py\", line 163, in _fetch\n",
      "    response = self._fetcher.fetch(full_url, body, self._connect_timeout)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/rpc.py\", line 116, in fetch\n",
      "    resp = self.session.post(full_url, data=body, timeout=timeout)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/sessions.py\", line 590, in post\n",
      "    return self.request('POST', url, data=data, json=json, **kwargs)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8082): Max retries exceeded with url: /api/add_task (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efdcc24fb50>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "WARNING: Failed connecting to remote scheduler 'http://localhost:8082'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 169, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/util/connection.py\", line 96, in create_connection\n",
      "    raise err\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/util/connection.py\", line 86, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 699, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 394, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 234, in request\n",
      "    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1255, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1301, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1250, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1010, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 950, in send\n",
      "    self.connect()\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 200, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 181, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7efdcc0ab070>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 755, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/util/retry.py\", line 573, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8082): Max retries exceeded with url: /api/ping (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efdcc0ab070>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/rpc.py\", line 163, in _fetch\n",
      "    response = self._fetcher.fetch(full_url, body, self._connect_timeout)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/rpc.py\", line 116, in fetch\n",
      "    resp = self.session.post(full_url, data=body, timeout=timeout)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/sessions.py\", line 590, in post\n",
      "    return self.request('POST', url, data=data, json=json, **kwargs)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8082): Max retries exceeded with url: /api/ping (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efdcc0ab070>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed connecting to remote scheduler 'http://localhost:8082'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 169, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/util/connection.py\", line 96, in create_connection\n",
      "    raise err\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/util/connection.py\", line 86, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 699, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 394, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 234, in request\n",
      "    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1255, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1301, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1250, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1010, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 950, in send\n",
      "    self.connect()\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 200, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 181, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7efdb7f604c0>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 755, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/util/retry.py\", line 573, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8082): Max retries exceeded with url: /api/add_task (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efdb7f604c0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/rpc.py\", line 163, in _fetch\n",
      "    response = self._fetcher.fetch(full_url, body, self._connect_timeout)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/rpc.py\", line 116, in fetch\n",
      "    resp = self.session.post(full_url, data=body, timeout=timeout)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/sessions.py\", line 590, in post\n",
      "    return self.request('POST', url, data=data, json=json, **kwargs)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8082): Max retries exceeded with url: /api/add_task (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efdb7f604c0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "ERROR: Luigi unexpected framework error while scheduling MyTask1(x=0)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/worker.py\", line 756, in add\n",
      "    for next in self._add(item, is_complete):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/worker.py\", line 861, in _add\n",
      "    self._add_task(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/worker.py\", line 585, in _add_task\n",
      "    self._scheduler.add_task(*args, **kwargs)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/scheduler.py\", line 114, in rpc_func\n",
      "    return self._request('/api/{}'.format(fn_name), actual_args, **request_args)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/rpc.py\", line 183, in _request\n",
      "    page = self._fetch(url, body)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/rpc.py\", line 172, in _fetch\n",
      "    raise RPCError(\n",
      "luigi.rpc.RPCError: Errors (3 attempts) when connecting to remote scheduler 'http://localhost:8082'\n",
      "WARNING: Failed connecting to remote scheduler 'http://localhost:8082'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 169, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/util/connection.py\", line 96, in create_connection\n",
      "    raise err\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/util/connection.py\", line 86, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 699, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 394, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 234, in request\n",
      "    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1255, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1301, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1250, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 1010, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/http/client.py\", line 950, in send\n",
      "    self.connect()\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 200, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connection.py\", line 181, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7efdcc0ab430>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 755, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/urllib3/util/retry.py\", line 573, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8082): Max retries exceeded with url: /api/ping (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efdcc0ab430>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/rpc.py\", line 163, in _fetch\n",
      "    response = self._fetcher.fetch(full_url, body, self._connect_timeout)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/rpc.py\", line 116, in fetch\n",
      "    resp = self.session.post(full_url, data=body, timeout=timeout)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/sessions.py\", line 590, in post\n",
      "    return self.request('POST', url, data=data, json=json, **kwargs)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/patrick/anaconda3/envs/patrick/lib/python3.9/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8082): Max retries exceeded with url: /api/ping (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efdcc0ab430>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed pinging scheduler\n"
     ]
    },
    {
     "ename": "RPCError",
     "evalue": "Errors (3 attempts) when connecting to remote scheduler 'http://localhost:8082'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRPCError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c6b463196153>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m#luigi.build([MyTask1(x=j), MyTask2(x=15,)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mluigi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMyTask1\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"end of jobs =\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/interface.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(tasks, worker_scheduler_factory, detailed_summary, **env_params)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0menv_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"no_lock\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0mluigi_run_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_schedule_and_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_scheduler_factory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverride_defaults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mluigi_run_result\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdetailed_summary\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mluigi_run_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduling_succeeded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/interface.py\u001b[0m in \u001b[0;36m_schedule_and_run\u001b[0;34m(tasks, worker_scheduler_factory, override_defaults)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0msuccess\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_scheduling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_scheduling_processes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done scheduling tasks'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0msuccess\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/worker.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, task, multiprocess, processes)\u001b[0m\n\u001b[1;32m    754\u001b[0m                 \u001b[0mqueue_size\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                 \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_complete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mnext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_complete\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_id\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/worker.py\u001b[0m in \u001b[0;36m_add\u001b[0;34m(self, task, is_complete)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scheduled_tasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m         self._add_task(\n\u001b[0m\u001b[1;32m    862\u001b[0m             \u001b[0mworker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0mtask_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/worker.py\u001b[0m in \u001b[0;36m_add_task\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'param_visibilities'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_param_visibilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Informed scheduler that task   %s   has status   %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/scheduler.py\u001b[0m in \u001b[0;36mrpc_func\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 raise TypeError('{} takes {} arguments ({} given)'.format(\n\u001b[1;32m    113\u001b[0m                     fn_name, len(all_args), len(actual_args)))\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/api/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mRPC_METHODS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpc_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/rpc.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, url, data, attempts, allow_null)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattempts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mallow_null\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/patrick/lib/python3.9/site-packages/luigi/rpc.py\u001b[0m in \u001b[0;36m_fetch\u001b[0;34m(self, url_suffix, body)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             raise RPCError(\n\u001b[0m\u001b[1;32m    173\u001b[0m                 \u001b[0;34m\"Errors (%d attempts) when connecting to remote scheduler %r\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rpc_retry_attempts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRPCError\u001b[0m: Errors (3 attempts) when connecting to remote scheduler 'http://localhost:8082'"
     ]
    }
   ],
   "source": [
    "import datetime, os, sys, json, luigi, time, bz2\n",
    "\n",
    "class MyTask1(luigi.Task):\n",
    "    x = luigi.Parameter() \n",
    "    \n",
    "\n",
    "    def run(self):\n",
    "        l = self.x\n",
    "        ll = bz2.decompress (l)\n",
    "        \n",
    "    def outputs(self,):\n",
    "        return\n",
    "\n",
    "\n",
    "class MyTask2(luigi.Task):\n",
    "    x = luigi.IntParameter()\n",
    "    y = luigi.IntParameter(default=1)\n",
    "    z = luigi.IntParameter(default=2)\n",
    "\n",
    "    def run(self):\n",
    "        print(self.x * self.y * self.z)\n",
    "    \n",
    "    def outputs(self,):\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "t = time.time()\n",
    "for i in range(0,10) :\n",
    "    #luigi.build([MyTask1(x=j), MyTask2(x=15,)])\n",
    "    luigi.build([MyTask1( x= i )])\n",
    "print (\"end of jobs =\",time.time ()- t)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import luigi\n",
    "\n",
    "\n",
    "class InputText(luigi.ExternalTask):\n",
    "    \"\"\"\n",
    "    This class represents something that was created elsewhere by an external process,\n",
    "    so all we want to do is to implement the output method.\n",
    "    \"\"\"\n",
    "    date = luigi.DateParameter()\n",
    "\n",
    "    def output(self):\n",
    "        \"\"\"\n",
    "        Returns the target output for this task.\n",
    "        In this case, it expects a file to be present in the local file system.\n",
    "        :return: the target output for this task.\n",
    "        :rtype: object (:py:class:`luigi.target.Target`)\n",
    "        \"\"\"\n",
    "        return luigi.LocalTarget(self.date.strftime('/var/tmp/text/%Y-%m-%d.txt'))\n",
    "\n",
    "\n",
    "class WordCount(luigi.Task):\n",
    "    date_interval = luigi.DateIntervalParameter()\n",
    "\n",
    "    def requires(self):\n",
    "        \"\"\"\n",
    "        This task's dependencies:\n",
    "        * :py:class:`~.InputText`\n",
    "        :return: list of object (:py:class:`luigi.task.Task`)\n",
    "        \"\"\"\n",
    "        return [InputText(date) for date in self.date_interval.dates()]\n",
    "\n",
    "    def output(self):\n",
    "        \"\"\"\n",
    "        Returns the target output for this task.\n",
    "        In this case, a successful execution of this task will create a file on the local filesystem.\n",
    "        :return: the target output for this task.\n",
    "        :rtype: object (:py:class:`luigi.target.Target`)\n",
    "        \"\"\"\n",
    "        return luigi.LocalTarget('/var/tmp/text-count/%s' % self.date_interval)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        1. count the words for each of the :py:meth:`~.InputText.output` targets created by :py:class:`~.InputText`\n",
    "        2. write the count into the :py:meth:`~.WordCount.output` target\n",
    "        \"\"\"\n",
    "        count = {}\n",
    "\n",
    "        # NOTE: self.input() actually returns an element for the InputText.output() target\n",
    "        for f in self.input():  # The input() method is a wrapper around requires() that returns Target objects\n",
    "            for line in f.open('r'):  # Target objects are a file system/format abstraction and this will return a file stream object\n",
    "                for word in line.strip().split():\n",
    "                    count[word] = count.get(word, 0) + 1\n",
    "\n",
    "        # output data\n",
    "        f = self.output().open('w')\n",
    "        for word, count in count.items():\n",
    "            f.write(\"%s\\t%d\\n\" % (word, count))\n",
    "        f.close()  # WARNING: file system operations are atomic therefore if you don't close the file you lose all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import luigi\n",
    "import os, sys, json, datetime\n",
    "\n",
    "class FakeDocuments(luigi.Task):\n",
    "    \"\"\"\n",
    "    Generates a local file containing 5 elements of data in JSON format.\n",
    "    \"\"\"\n",
    "\n",
    "    #: the date parameter.\n",
    "    date = luigi.DateParameter(default=datetime.date.today())\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Writes data in JSON format into the task's output target.\n",
    "\n",
    "        The data objects have the following attributes:\n",
    "\n",
    "        * `_id` is the default Elasticsearch id field,\n",
    "        * `text`: the text,\n",
    "        * `date`: the day when the data was created.\n",
    "\n",
    "        \"\"\"\n",
    "        print ('run Fake original')\n",
    "        today = datetime.date.today()\n",
    "        with self.output().open('w') as output:\n",
    "            for i in range(5):\n",
    "                output.write(json.dumps({'_id': i, 'text': 'Hi %s' % i,\n",
    "                                         'date': str(today)}))\n",
    "                output.write('\\n')\n",
    "        \n",
    "    def output(self):\n",
    "        \"\"\"\n",
    "        Returns the target output for this task.\n",
    "        In this case, a successful execution of this task will create a file on the local filesystem.\n",
    "\n",
    "        :return: the target output for this task.\n",
    "        :rtype: object (:py:class:`luigi.target.Target`)\n",
    "        \"\"\"\n",
    "        return luigi.LocalTarget('docs-%s.ldj' % self.date)\n",
    "    \n",
    "    def requires(self):\n",
    "        return []\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class Appel(luigi.Task):\n",
    "    \n",
    "    nameOut =  luigi.Parameter()\n",
    "    \n",
    "    def requires(self):\n",
    "        return [FakeDocuments()]\n",
    "    \n",
    "    def output(self):\n",
    "        return luigi.LocalTarget()\n",
    "     \n",
    "    def run(self):\n",
    "        print ('run final')\n",
    "        with self.input()[0].open() as fin, self.output().open('w') as fout:\n",
    "            for line in fin:\n",
    "                print ('ecriture')\n",
    "                fout.write(\"{}\\n\".format(line))\n",
    "\n",
    "                \n",
    "\n",
    " \n",
    "    \n",
    "        \n",
    "        \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "name = 'resultat.txt'\n",
    "\n",
    "\n",
    "\n",
    "local_scheduler = True\n",
    "workers = 5\n",
    "if local_scheduler is False:\n",
    "    luigi.build(Appel(nameOut = name), workers=workers, scheduler_host=luigi_host)\n",
    "else:\n",
    "    luigi.build(Appel(name), workers=workers,  )\n",
    "    \n",
    "if os.existe (name) :\n",
    "    os.sys.delete (name)\n",
    "\n",
    "                \n",
    "print (\"end of jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import luigi\n",
    "from luigi.contrib.esindex import CopyToIndex\n",
    "\n",
    "class ExampleIndex(CopyToIndex):\n",
    "    index = 'example'\n",
    "\n",
    "    def docs(self):\n",
    "        return [{'_id': 1, 'title': 'An example document.'}]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    task = ExampleIndex()\n",
    "    luigi.build([task], local_scheduler=True)\n",
    "    \n",
    "print (\"end of jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import luigi\n",
    "from luigi.contrib.esindex import CopyToIndex\n",
    "\n",
    "\n",
    "class ExampleIndex(CopyToIndex):\n",
    "    host = 'localhost'\n",
    "    port = 9200\n",
    "    index = 'example'\n",
    "    #doc_type = 'default' # pourquoi ????? cela sert -t-il \n",
    "    purge_existing_index = True\n",
    "    marker_index_hist_size = 1\n",
    "\n",
    "    def docs(self):\n",
    "        return [{'_id': 1, 'title': 'An example document.'}]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    task = ExampleIndex()\n",
    "    luigi.build([task], local_scheduler=True)\n",
    "    \n",
    "\n",
    "print (\"end of jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Copyright 2012-2015 Spotify AB\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "import luigi\n",
    "from luigi.contrib.esindex import CopyToIndex\n",
    "from time import time\n",
    "\n",
    "class FakeDocuments(luigi.Task):\n",
    "    \"\"\"\n",
    "    Generates a local file containing 5 elements of data in JSON format.\n",
    "    \"\"\"\n",
    "\n",
    "    #: the date parameter.\n",
    "    date = luigi.DateParameter(default=datetime.date.today())\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Writes data in JSON format into the task's output target.\n",
    "\n",
    "        The data objects have the following attributes:\n",
    "\n",
    "        * `_id` is the default Elasticsearch id field,\n",
    "        * `text`: the text,\n",
    "        * `date`: the day when the data was created.\n",
    "\n",
    "        \"\"\"\n",
    "        print ('run Fake')\n",
    "        today = datetime.date.today()\n",
    "        with self.output().open('w') as output:\n",
    "            for i in range(5):\n",
    "                output.write(json.dumps({'_id': i, 'text': 'Hi %s' % i,\n",
    "                                         'date': str(today)}))\n",
    "                output.write('\\n')\n",
    "            \n",
    "\n",
    "    def output(self):\n",
    "        \"\"\"\n",
    "        Returns the target output for this task.\n",
    "        In this case, a successful execution of this task will create a file on the local filesystem.\n",
    "\n",
    "        :return: the target output for this task.\n",
    "        :rtype: object (:py:class:`luigi.target.Target`)\n",
    "        \"\"\"\n",
    "        return luigi.LocalTarget(path='./_docs-%s.ldj' % self.date)\n",
    "\n",
    "\n",
    "class IndexDocuments(CopyToIndex):\n",
    "    \"\"\"\n",
    "    This task loads JSON data contained in a :py:class:`luigi.target.Target` into an ElasticSearch index.\n",
    "\n",
    "    This task's input will the target returned by :py:meth:`~.FakeDocuments.output`.\n",
    "\n",
    "    This class uses :py:meth:`luigi.contrib.esindex.CopyToIndex.run`.\n",
    "\n",
    "    After running this task you can run:\n",
    "\n",
    "    .. code-block:: console\n",
    "\n",
    "        $ curl \"localhost:9200/example_index/_search?pretty\"\n",
    "\n",
    "    to see the indexed documents.\n",
    "\n",
    "    To see the update log, run\n",
    "\n",
    "    .. code-block:: console\n",
    "\n",
    "        $ curl \"localhost:9200/update_log/_search?q=target_index:example_index&pretty\"\n",
    "\n",
    "    To cleanup both indexes run:\n",
    "\n",
    "    .. code-block:: console\n",
    "\n",
    "        $ curl -XDELETE \"localhost:9200/example_index\"\n",
    "        $ curl -XDELETE \"localhost:9200/update_log/_query?q=target_index:example_index\"\n",
    "\n",
    "    \"\"\"\n",
    "    host = 'localhost'\n",
    "    port = 9200\n",
    "    index = 'example'\n",
    "    #doc_type = 'default' # pourquoi ????? cela sert -t-il dans elastic 7.0 modif de .../config/esindex.py\n",
    "    purge_existing_index = True\n",
    "    marker_index_hist_size = 1\n",
    "    \n",
    "    #: date task parameter (default = today)\n",
    "    date = luigi.DateParameter(default=datetime.date.today())\n",
    "\n",
    "    #: the name of the index in ElasticSearch to be updated.\n",
    "    index = 'example_index'\n",
    "    #: the name of the document type.\n",
    "    doc_type = 'greetings'\n",
    "    #: the host running the ElasticSearch service.\n",
    "    host = 'localhost'\n",
    "    #: the port used by the ElasticSearch service.\n",
    "    port = 9200\n",
    "\n",
    "    def requires(self):\n",
    "        \"\"\"\n",
    "        This task's dependencies:\n",
    "\n",
    "        * :py:class:`~.FakeDocuments`\n",
    "\n",
    "        :return: object (:py:class:`luigi.task.Task`)\n",
    "        \"\"\"\n",
    "        return FakeDocuments()\n",
    "    \n",
    "    def docs(self):\n",
    "        \"\"\"\n",
    "        Return the documents to be indexed.\n",
    "        Beside the user defined fields, the document may contain an `_index`, `_type` and `_id`.\n",
    "        \"\"\"\n",
    "        print ('doc')\n",
    "        with self.input().open('r') as fobj:\n",
    "            for line in fobj:\n",
    "                yield line\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    commentaire sur le run pour CopyToIndex : vitesse ou comportement run task?\n",
    "    purge existing index, if requested (purge_existing_index),\n",
    "    create the index, if missing,\n",
    "    apply mappings, if given,\n",
    "    set refresh interval to -1 (disable) for performance reasons,\n",
    "    bulk index in batches of size chunk_size (2000),\n",
    "    set refresh interval to 1s,\n",
    "    refresh Elasticsearch,\n",
    "    create entry in marker index.\n",
    "    \"\"\"\n",
    "    t = time ()\n",
    "    task = IndexDocuments ()\n",
    "    luigi.build([task], local_scheduler=True)\n",
    "    tf = time() -t\n",
    "    print (\"end of job en \", tf, \" secondes\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Copyright 2012-2015 Spotify AB\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\"\"\"\n",
    "Support for Elasticsearch (1.0.0 or newer).\n",
    "Provides an :class:`ElasticsearchTarget` and a :class:`CopyToIndex` template task.\n",
    "Modeled after :class:`luigi.contrib.rdbms.CopyToTable`.\n",
    "A minimal example (assuming elasticsearch is running on localhost:9200):\n",
    ".. code-block:: python\n",
    "    class ExampleIndex(CopyToIndex):\n",
    "        index = 'example'\n",
    "        def docs(self):\n",
    "            return [{'_id': 1, 'title': 'An example document.'}]\n",
    "    if __name__ == '__main__':\n",
    "        task = ExampleIndex()\n",
    "        luigi.build([task], local_scheduler=True)\n",
    "All options:\n",
    ".. code-block:: python\n",
    "    class ExampleIndex(CopyToIndex):\n",
    "        host = 'localhost'\n",
    "        port = 9200\n",
    "        index = 'example'\n",
    "        doc_type = 'default'\n",
    "        purge_existing_index = True\n",
    "        marker_index_hist_size = 1\n",
    "        def docs(self):\n",
    "            return [{'_id': 1, 'title': 'An example document.'}]\n",
    "    if __name__ == '__main__':\n",
    "        task = ExampleIndex()\n",
    "        luigi.build([task], local_scheduler=True)\n",
    "`Host`, `port`, `index`, `doc_type` parameters are standard elasticsearch.\n",
    "`purge_existing_index` will delete the index, whenever an update is required.\n",
    "This is useful, when one deals with \"dumps\" that represent the whole data, not just updates.\n",
    "`marker_index_hist_size` sets the maximum number of entries in the 'marker'\n",
    "index:\n",
    "* 0 (default) keeps all updates,\n",
    "* 1 to only remember the most recent update to the index.\n",
    "This can be useful, if an index needs to recreated, even though\n",
    "the corresponding indexing task has been run sometime in the past - but\n",
    "a later indexing task might have altered the index in the meantime.\n",
    "There are a two luigi `luigi.cfg` configuration options:\n",
    ".. code-block:: ini\n",
    "    [elasticsearch]\n",
    "    marker-index = update_log\n",
    "    marker-doc-type = entry\n",
    "\"\"\"\n",
    "\n",
    "# pylint: disable=F0401,E1101,C0103\n",
    "import abc\n",
    "import datetime\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import itertools\n",
    "\n",
    "import luigi\n",
    "\n",
    "logger = logging.getLogger('luigi-interface')\n",
    "\n",
    "try:\n",
    "    import elasticsearch\n",
    "    if elasticsearch.__version__ < (1, 0, 0):\n",
    "        logger.warning(\"This module works with elasticsearch 1.0.0 \"\n",
    "                       \"or newer only.\")\n",
    "    from elasticsearch.helpers import bulk\n",
    "    from elasticsearch.connection import Urllib3HttpConnection\n",
    "\n",
    "except ImportError:\n",
    "    logger.warning(\"Loading esindex module without elasticsearch installed. \"\n",
    "                   \"Will crash at runtime if esindex functionality is used.\")\n",
    "\n",
    "\n",
    "class ElasticsearchTarget(luigi.Target):\n",
    "    \"\"\" Target for a resource in Elasticsearch.\"\"\"\n",
    "\n",
    "    marker_index = luigi.configuration.get_config().get('elasticsearch',\n",
    "                                                        'marker-index', 'update_log')\n",
    "    marker_doc_type = luigi.configuration.get_config().get('elasticsearch',\n",
    "                                                           'marker-doc-type', 'entry')\n",
    "\n",
    "    def __init__(self, host, port, index, doc_type, update_id,\n",
    "                 marker_index_hist_size=0, http_auth=None, timeout=10,\n",
    "                 extra_elasticsearch_args=None):\n",
    "        \"\"\"\n",
    "        :param host: Elasticsearch server host\n",
    "        :type host: str\n",
    "        :param port: Elasticsearch server port\n",
    "        :type port: int\n",
    "        :param index: index name\n",
    "        :type index: str\n",
    "        :param doc_type: doctype name\n",
    "        :type doc_type: str\n",
    "        :param update_id: an identifier for this data set\n",
    "        :type update_id: str\n",
    "        :param marker_index_hist_size: list of changes to the index to remember\n",
    "        :type marker_index_hist_size: int\n",
    "        :param timeout: Elasticsearch connection timeout\n",
    "        :type timeout: int\n",
    "        :param extra_elasticsearch_args: extra args for Elasticsearch\n",
    "        :type Extra: dict\n",
    "        \"\"\"\n",
    "        if extra_elasticsearch_args is None:\n",
    "            extra_elasticsearch_args = {}\n",
    "\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.http_auth = http_auth\n",
    "        self.index = index\n",
    "        self.doc_type = doc_type\n",
    "        self.update_id = update_id\n",
    "        self.marker_index_hist_size = marker_index_hist_size\n",
    "        self.timeout = timeout\n",
    "        self.extra_elasticsearch_args = extra_elasticsearch_args\n",
    "\n",
    "        self.es = elasticsearch.Elasticsearch(\n",
    "            connection_class=Urllib3HttpConnection,\n",
    "            host=self.host,\n",
    "            port=self.port,\n",
    "            http_auth=self.http_auth,\n",
    "            timeout=self.timeout,\n",
    "            **self.extra_elasticsearch_args\n",
    "        )\n",
    "\n",
    "    def marker_index_document_id(self):\n",
    "        \"\"\"\n",
    "        Generate an id for the indicator document.\n",
    "        \"\"\"\n",
    "        params = '%s:%s:%s' % (self.index, self.doc_type, self.update_id)\n",
    "        return hashlib.sha1(params.encode('utf-8')).hexdigest()\n",
    "\n",
    "    def touch(self):\n",
    "        \"\"\"\n",
    "        Mark this update as complete.\n",
    "        The document id would be sufficient but,\n",
    "        for documentation,\n",
    "        we index the parameters `update_id`, `target_index`, `target_doc_type` and `date` as well.\n",
    "        \"\"\"\n",
    "        self.create_marker_index()\n",
    "        self.es.index(index=self.marker_index, doc_type=self.marker_doc_type,\n",
    "                      id=self.marker_index_document_id(), body={\n",
    "                          'update_id': self.update_id,\n",
    "                          'target_index': self.index,\n",
    "                          'target_doc_type': self.doc_type,\n",
    "                          'date': datetime.datetime.now()})\n",
    "        self.es.indices.flush(index=self.marker_index)\n",
    "        self.ensure_hist_size()\n",
    "\n",
    "    def exists(self):\n",
    "        \"\"\"\n",
    "        Test, if this task has been run.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.es.get(index=self.marker_index, doc_type=self.marker_doc_type, id=self.marker_index_document_id())\n",
    "            return True\n",
    "        except elasticsearch.NotFoundError:\n",
    "            logger.debug('Marker document not found.')\n",
    "        except elasticsearch.ElasticsearchException as err:\n",
    "            logger.warn(err)\n",
    "        return False\n",
    "\n",
    "    def create_marker_index(self):\n",
    "        \"\"\"\n",
    "        Create the index that will keep track of the tasks if necessary.\n",
    "        \"\"\"\n",
    "        if not self.es.indices.exists(index=self.marker_index):\n",
    "            self.es.indices.create(index=self.marker_index)\n",
    "\n",
    "    def ensure_hist_size(self):\n",
    "        \"\"\"\n",
    "        Shrink the history of updates for\n",
    "        a `index/doc_type` combination down to `self.marker_index_hist_size`.\n",
    "        \"\"\"\n",
    "        if self.marker_index_hist_size == 0:\n",
    "            return\n",
    "        result = self.es.search(index=self.marker_index,\n",
    "                                doc_type=self.marker_doc_type,\n",
    "                                body={'query': {\n",
    "                                    'term': {'target_index': self.index}}},\n",
    "                                sort=('date:desc',))\n",
    "\n",
    "        for i, hit in enumerate(result.get('hits').get('hits'), start=1):\n",
    "            if i > self.marker_index_hist_size:\n",
    "                marker_document_id = hit.get('_id')\n",
    "                self.es.delete(id=marker_document_id, index=self.marker_index,\n",
    "                               doc_type=self.marker_doc_type)\n",
    "        self.es.indices.flush(index=self.marker_index)\n",
    "\n",
    "\n",
    "class CopyToIndex(luigi.Task):\n",
    "    \"\"\"\n",
    "    Template task for inserting a data set into Elasticsearch.\n",
    "    Usage:\n",
    "    1. Subclass and override the required `index` attribute.\n",
    "    2. Implement a custom `docs` method, that returns an iterable over the documents.\n",
    "       A document can be a JSON string,\n",
    "       e.g. from a newline-delimited JSON (ldj) file (default implementation)\n",
    "       or some dictionary.\n",
    "    Optional attributes:\n",
    "    * doc_type (default),\n",
    "    * host (localhost),\n",
    "    * port (9200),\n",
    "    * settings ({'settings': {}})\n",
    "    * mapping (None),\n",
    "    * chunk_size (2000),\n",
    "    * raise_on_error (True),\n",
    "    * purge_existing_index (False),\n",
    "    * marker_index_hist_size (0)\n",
    "    If settings are defined, they are only applied at index creation time.\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def host(self):\n",
    "        \"\"\"\n",
    "        ES hostname.\n",
    "        \"\"\"\n",
    "        return 'localhost'\n",
    "\n",
    "    @property\n",
    "    def port(self):\n",
    "        \"\"\"\n",
    "        ES port.\n",
    "        \"\"\"\n",
    "        return 9200\n",
    "\n",
    "    @property\n",
    "    def http_auth(self):\n",
    "        \"\"\"\n",
    "        ES optional http auth information as either â:â separated string or a tuple,\n",
    "        e.g. `('user', 'pass')` or `\"user:pass\"`.\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def index(self):\n",
    "        \"\"\"\n",
    "        The target index.\n",
    "        May exist or not.\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def doc_type(self):\n",
    "        \"\"\"\n",
    "        The target doc_type.\n",
    "        \"\"\"\n",
    "        return 'default'\n",
    "\n",
    "    @property\n",
    "    def mapping(self):\n",
    "        \"\"\"\n",
    "        Dictionary with custom mapping or `None`.\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def settings(self):\n",
    "        \"\"\"\n",
    "        Settings to be used at index creation time.\n",
    "        \"\"\"\n",
    "        return {'settings': {}}\n",
    "\n",
    "    @property\n",
    "    def chunk_size(self):\n",
    "        \"\"\"\n",
    "        Single API call for this number of docs.\n",
    "        \"\"\"\n",
    "        return 2000\n",
    "\n",
    "    @property\n",
    "    def raise_on_error(self):\n",
    "        \"\"\"\n",
    "        Whether to fail fast.\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def purge_existing_index(self):\n",
    "        \"\"\"\n",
    "        Whether to delete the `index` completely before any indexing.\n",
    "        \"\"\"\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def marker_index_hist_size(self):\n",
    "        \"\"\"\n",
    "        Number of event log entries in the marker index. 0: unlimited.\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def timeout(self):\n",
    "        \"\"\"\n",
    "        Timeout.\n",
    "        \"\"\"\n",
    "        return 10\n",
    "\n",
    "    @property\n",
    "    def extra_elasticsearch_args(self):\n",
    "        \"\"\"\n",
    "        Extra arguments to pass to the Elasticsearch constructor\n",
    "        \"\"\"\n",
    "        return {}\n",
    "\n",
    "    def docs(self):\n",
    "        \"\"\"\n",
    "        Return the documents to be indexed.\n",
    "        Beside the user defined fields, the document may contain an `_index`, `_type` and `_id`.\n",
    "        \"\"\"\n",
    "        with self.input().open('r') as fobj:\n",
    "            for line in fobj:\n",
    "                yield line\n",
    "\n",
    "# everything below will rarely have to be overridden\n",
    "\n",
    "    def _docs(self):\n",
    "        \"\"\"\n",
    "        Since `self.docs` may yield documents that do not explicitly contain `_index` or `_type`,\n",
    "        add those attributes here, if necessary.\n",
    "        \"\"\"\n",
    "        iterdocs = iter(self.docs())\n",
    "        first = next(iterdocs)\n",
    "        needs_parsing = False\n",
    "        if isinstance(first, str):\n",
    "            needs_parsing = True\n",
    "        elif isinstance(first, dict):\n",
    "            pass\n",
    "        else:\n",
    "            raise RuntimeError('Document must be either JSON strings or dict.')\n",
    "        for doc in itertools.chain([first], iterdocs):\n",
    "            if needs_parsing:\n",
    "                doc = json.loads(doc)\n",
    "            if '_index' not in doc:\n",
    "                doc['_index'] = self.index\n",
    "            if '_type' not in doc:\n",
    "                doc['_type'] = self.doc_type\n",
    "            yield doc\n",
    "\n",
    "    def _init_connection(self):\n",
    "        return elasticsearch.Elasticsearch(\n",
    "            connection_class=Urllib3HttpConnection,\n",
    "            host=self.host,\n",
    "            port=self.port,\n",
    "            http_auth=self.http_auth,\n",
    "            timeout=self.timeout,\n",
    "            **self.extra_elasticsearch_args\n",
    "        )\n",
    "\n",
    "    def create_index(self):\n",
    "        \"\"\"\n",
    "        Override to provide code for creating the target index.\n",
    "        By default it will be created without any special settings or mappings.\n",
    "        \"\"\"\n",
    "        es = self._init_connection()\n",
    "        if not es.indices.exists(index=self.index):\n",
    "            es.indices.create(index=self.index, body=self.settings)\n",
    "\n",
    "    def delete_index(self):\n",
    "        \"\"\"\n",
    "        Delete the index, if it exists.\n",
    "        \"\"\"\n",
    "        es = self._init_connection()\n",
    "        if es.indices.exists(index=self.index):\n",
    "            es.indices.delete(index=self.index)\n",
    "\n",
    "    def update_id(self):\n",
    "        \"\"\"\n",
    "        This id will be a unique identifier for this indexing task.\n",
    "        \"\"\"\n",
    "        return self.task_id\n",
    "\n",
    "    def output(self):\n",
    "        \"\"\"\n",
    "        Returns a ElasticsearchTarget representing the inserted dataset.\n",
    "        Normally you don't override this.\n",
    "        \"\"\"\n",
    "        return ElasticsearchTarget(\n",
    "            host=self.host,\n",
    "            port=self.port,\n",
    "            http_auth=self.http_auth,\n",
    "            index=self.index,\n",
    "            doc_type=self.doc_type,\n",
    "            update_id=self.update_id(),\n",
    "            marker_index_hist_size=self.marker_index_hist_size,\n",
    "            timeout=self.timeout,\n",
    "            extra_elasticsearch_args=self.extra_elasticsearch_args\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run task, namely:\n",
    "        * purge existing index, if requested (`purge_existing_index`),\n",
    "        * create the index, if missing,\n",
    "        * apply mappings, if given,\n",
    "        * set refresh interval to -1 (disable) for performance reasons,\n",
    "        * bulk index in batches of size `chunk_size` (2000),\n",
    "        * set refresh interval to 1s,\n",
    "        * refresh Elasticsearch,\n",
    "        * create entry in marker index.\n",
    "        \"\"\"\n",
    "        if self.purge_existing_index:\n",
    "            self.delete_index()\n",
    "        self.create_index()\n",
    "        es = self._init_connection()\n",
    "        if self.mapping:\n",
    "            es.indices.put_mapping(index=self.index, doc_type=self.doc_type,\n",
    "                                   body=self.mapping)\n",
    "        es.indices.put_settings({\"index\": {\"refresh_interval\": \"-1\"}},\n",
    "                                index=self.index)\n",
    "\n",
    "        bulk(es, self._docs(), chunk_size=self.chunk_size,\n",
    "             raise_on_error=self.raise_on_error)\n",
    "\n",
    "        es.indices.put_settings({\"index\": {\"refresh_interval\": \"1s\"}},\n",
    "                                index=self.index)\n",
    "        es.indices.refresh()\n",
    "        self.output().touch()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Copyright 2012-2015 Spotify AB\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# modification appel de ES sans doc_type\n",
    "\"\"\"\n",
    "Support for Elasticsearch (1.0.0 or newer).\n",
    "\n",
    "Provides an :class:`ElasticsearchTarget` and a :class:`CopyToIndex` template task.\n",
    "\n",
    "Modeled after :class:`luigi.contrib.rdbms.CopyToTable`.\n",
    "\n",
    "A minimal example (assuming elasticsearch is running on localhost:9200):\n",
    "\n",
    ".. code-block:: python\n",
    "\n",
    "    class ExampleIndex(CopyToIndex):\n",
    "        index = 'example'\n",
    "\n",
    "        def docs(self):\n",
    "            return [{'_id': 1, 'title': 'An example document.'}]\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        task = ExampleIndex()\n",
    "        luigi.build([task], local_scheduler=True)\n",
    "\n",
    "All options:\n",
    "\n",
    ".. code-block:: python\n",
    "\n",
    "    class ExampleIndex(CopyToIndex):\n",
    "        host = 'localhost'\n",
    "        port = 9200\n",
    "        index = 'example'\n",
    "        doc_type = 'default'\n",
    "        purge_existing_index = True\n",
    "        marker_index_hist_size = 1\n",
    "\n",
    "        def docs(self):\n",
    "            return [{'_id': 1, 'title': 'An example document.'}]\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        task = ExampleIndex()\n",
    "        luigi.build([task], local_scheduler=True)\n",
    "\n",
    "`Host`, `port`, `index`, `doc_type` parameters are standard elasticsearch.\n",
    "\n",
    "`purge_existing_index` will delete the index, whenever an update is required.\n",
    "This is useful, when one deals with \"dumps\" that represent the whole data, not just updates.\n",
    "\n",
    "`marker_index_hist_size` sets the maximum number of entries in the 'marker'\n",
    "index:\n",
    "\n",
    "* 0 (default) keeps all updates,\n",
    "* 1 to only remember the most recent update to the index.\n",
    "\n",
    "This can be useful, if an index needs to recreated, even though\n",
    "the corresponding indexing task has been run sometime in the past - but\n",
    "a later indexing task might have altered the index in the meantime.\n",
    "\n",
    "There are a two luigi `luigi.cfg` configuration options:\n",
    "\n",
    ".. code-block:: ini\n",
    "\n",
    "    [elasticsearch]\n",
    "\n",
    "    marker-index = update_log\n",
    "    marker-doc-type = entry\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# pylint: disable=F0401,E1101,C0103\n",
    "import abc\n",
    "import datetime\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import itertools\n",
    "\n",
    "import luigi\n",
    "\n",
    "logger = logging.getLogger('luigi-interface')\n",
    "\n",
    "try:\n",
    "    import elasticsearch\n",
    "    if elasticsearch.__version__ < (1, 0, 0):\n",
    "        logger.warning(\"This module works with elasticsearch 1.0.0 \"\n",
    "                       \"or newer only.\")\n",
    "    from elasticsearch.helpers import bulk\n",
    "    from elasticsearch.connection import Urllib3HttpConnection\n",
    "\n",
    "except ImportError:\n",
    "    logger.warning(\"Loading esindex module without elasticsearch installed. \"\n",
    "                   \"Will crash at runtime if esindex functionality is used.\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "class ElasticsearchTarget(luigi.Target):\n",
    "    \n",
    "    \n",
    "    \"\"\" Target for a resource in Elasticsearch.\"\"\"\n",
    "\n",
    "    marker_index = luigi.configuration.get_config().get('elasticsearch',\n",
    "                                                        'marker-index', 'update_log')\n",
    "    marker_doc_type = luigi.configuration.get_config().get('elasticsearch',\n",
    "                                                           'marker-doc-type', 'entry')\n",
    "    \n",
    "\n",
    "    def __init__(self, host, port, index, doc_type, update_id,\n",
    "                 marker_index_hist_size=0, http_auth=None, timeout=10,\n",
    "                 extra_elasticsearch_args=None):\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "#    def __init__(self, host, port, index, update_id,\n",
    "#                 marker_index_hist_size=0, http_auth=None, timeout=10,\n",
    "#                 extra_elasticsearch_args=None):\n",
    "\n",
    "        \n",
    "        # essai\n",
    "        \"\"\"\n",
    "        :param host: Elasticsearch server host\n",
    "        :type host: str\n",
    "        :param port: Elasticsearch server port\n",
    "        :type port: int\n",
    "        :param index: index name\n",
    "        :type index: str\n",
    "        :param doc_type: doctype name\n",
    "        :type doc_type: str\n",
    "        :param update_id: an identifier for this data set\n",
    "        :type update_id: str\n",
    "        :param marker_index_hist_size: list of changes to the index to remember\n",
    "        :type marker_index_hist_size: int\n",
    "        :param timeout: Elasticsearch connection timeout\n",
    "        :type timeout: int\n",
    "        :param extra_elasticsearch_args: extra args for Elasticsearch\n",
    "        :type Extra: dict\n",
    "        \"\"\"\n",
    "        \n",
    "        if extra_elasticsearch_args is None:\n",
    "            extra_elasticsearch_args = {}\n",
    "\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.http_auth = http_auth\n",
    "        self.index = index\n",
    "        ########## modif\n",
    "        #self.doc_type = doc_type\n",
    "        self.update_id = update_id\n",
    "        self.marker_index_hist_size = marker_index_hist_size\n",
    "        self.timeout = timeout\n",
    "        self.extra_elasticsearch_args = extra_elasticsearch_args\n",
    "\n",
    "        self.es = elasticsearch.Elasticsearch(\n",
    "            connection_class=Urllib3HttpConnection,\n",
    "            host=self.host,\n",
    "            port=self.port,\n",
    "            http_auth=self.http_auth,\n",
    "            timeout=self.timeout,\n",
    "            **self.extra_elasticsearch_args\n",
    "        )\n",
    "        return\n",
    "    \n",
    "    def marker_index_document_id (self) :\n",
    "        return\n",
    "    \n",
    "    def marker_index_document_id(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Generate an id for the indicator document.\n",
    "        \"\"\"\n",
    "        \n",
    "        #### modif\n",
    "        #params = '%s:%s:%s' % (self.index, self.doc_type, self.update_id)\n",
    "        \n",
    "        params = '%s:%s' % (self.index, self.update_id)\n",
    "        \n",
    "        return hashlib.sha1(params.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Copyright 2012-2015 Spotify AB\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# modification appel de ES sans doc_type\n",
    "\"\"\"\n",
    "Support for Elasticsearch (1.0.0 or newer).\n",
    "\n",
    "Provides an :class:`ElasticsearchTarget` and a :class:`CopyToIndex` template task.\n",
    "\n",
    "Modeled after :class:`luigi.contrib.rdbms.CopyToTable`.\n",
    "\n",
    "A minimal example (assuming elasticsearch is running on localhost:9200):\n",
    "\n",
    ".. code-block:: python\n",
    "\n",
    "    class ExampleIndex(CopyToIndex):\n",
    "        index = 'example'\n",
    "\n",
    "        def docs(self):\n",
    "            return [{'_id': 1, 'title': 'An example document.'}]\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        task = ExampleIndex()\n",
    "        luigi.build([task], local_scheduler=True)\n",
    "\n",
    "All options:\n",
    "\n",
    ".. code-block:: python\n",
    "\n",
    "    class ExampleIndex(CopyToIndex):\n",
    "        host = 'localhost'\n",
    "        port = 9200\n",
    "        index = 'example'\n",
    "        doc_type = 'default'\n",
    "        purge_existing_index = True\n",
    "        marker_index_hist_size = 1\n",
    "\n",
    "        def docs(self):\n",
    "            return [{'_id': 1, 'title': 'An example document.'}]\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        task = ExampleIndex()\n",
    "        luigi.build([task], local_scheduler=True)\n",
    "\n",
    "`Host`, `port`, `index`, `doc_type` parameters are standard elasticsearch.\n",
    "\n",
    "`purge_existing_index` will delete the index, whenever an update is required.\n",
    "This is useful, when one deals with \"dumps\" that represent the whole data, not just updates.\n",
    "\n",
    "`marker_index_hist_size` sets the maximum number of entries in the 'marker'\n",
    "index:\n",
    "\n",
    "* 0 (default) keeps all updates,\n",
    "* 1 to only remember the most recent update to the index.\n",
    "\n",
    "This can be useful, if an index needs to recreated, even though\n",
    "the corresponding indexing task has been run sometime in the past - but\n",
    "a later indexing task might have altered the index in the meantime.\n",
    "\n",
    "There are a two luigi `luigi.cfg` configuration options:\n",
    "\n",
    ".. code-block:: ini\n",
    "\n",
    "    [elasticsearch]\n",
    "\n",
    "    marker-index = update_log\n",
    "    marker-doc-type = entry\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# pylint: disable=F0401,E1101,C0103\n",
    "import abc\n",
    "import datetime\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import itertools\n",
    "\n",
    "import luigi\n",
    "\n",
    "logger = logging.getLogger('luigi-interface')\n",
    "\n",
    "try:\n",
    "    import elasticsearch\n",
    "    if elasticsearch.__version__ < (1, 0, 0):\n",
    "        logger.warning(\"This module works with elasticsearch 1.0.0 \"\n",
    "                       \"or newer only.\")\n",
    "    from elasticsearch.helpers import bulk\n",
    "    from elasticsearch.connection import Urllib3HttpConnection\n",
    "\n",
    "except ImportError:\n",
    "    logger.warning(\"Loading esindex module without elasticsearch installed. \"\n",
    "                   \"Will crash at runtime if esindex functionality is used.\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "class ElasticsearchTarget(luigi.Target):\n",
    "    \n",
    "    \n",
    "    \"\"\" Target for a resource in Elasticsearch.\"\"\"\n",
    "\n",
    "    marker_index = luigi.configuration.get_config().get('elasticsearch',\n",
    "                                                        'marker-index', 'update_log')\n",
    "    marker_doc_type = luigi.configuration.get_config().get('elasticsearch',\n",
    "                                                           'marker-doc-type', 'entry')\n",
    "###modif    \n",
    "\n",
    "#    def __init__(self, host, port, index, doc_type, update_id,\n",
    "#                 marker_index_hist_size=0, http_auth=None, timeout=10,\n",
    "#                 extra_elasticsearch_args=None):\n",
    "        \n",
    "\n",
    "    def __init__(self, host, port, index, update_id,\n",
    "                 marker_index_hist_size=0, http_auth=None, timeout=10,\n",
    "                 extra_elasticsearch_args=None):\n",
    "\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        :param host: Elasticsearch server host\n",
    "        :type host: str\n",
    "        :param port: Elasticsearch server port\n",
    "        :type port: int\n",
    "        :param index: index name\n",
    "        :type index: str\n",
    "        :param doc_type: doctype name\n",
    "        :type doc_type: str\n",
    "        :param update_id: an identifier for this data set\n",
    "        :type update_id: str\n",
    "        :param marker_index_hist_size: list of changes to the index to remember\n",
    "        :type marker_index_hist_size: int\n",
    "        :param timeout: Elasticsearch connection timeout\n",
    "        :type timeout: int\n",
    "        :param extra_elasticsearch_args: extra args for Elasticsearch\n",
    "        :type Extra: dict\n",
    "        \"\"\"\n",
    "        \n",
    "        if extra_elasticsearch_args is None:\n",
    "            extra_elasticsearch_args = {}\n",
    "\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.http_auth = http_auth\n",
    "        self.index = index\n",
    "        ########## modif\n",
    "        #self.doc_type = doc_type\n",
    "        self.update_id = update_id\n",
    "        self.marker_index_hist_size = marker_index_hist_size\n",
    "        self.timeout = timeout\n",
    "        self.extra_elasticsearch_args = extra_elasticsearch_args\n",
    "\n",
    "        self.es = elasticsearch.Elasticsearch(\n",
    "            connection_class=Urllib3HttpConnection,\n",
    "            host=self.host,\n",
    "            port=self.port,\n",
    "            http_auth=self.http_auth,\n",
    "            timeout=self.timeout,\n",
    "            **self.extra_elasticsearch_args\n",
    "        )\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def marker_index_document_id(self):\n",
    "        \"\"\"\n",
    "        Generate an id for the indicator document.\n",
    "        \"\"\"\n",
    "        #### modif\n",
    "        #params = '%s:%s:%s' % (self.index, self.doc_type, self.update_id)\n",
    "        params = '%s:%s' % (self.index, self.update_id)\n",
    "        return hashlib.sha1(params.encode('utf-8')).hexdigest()\n",
    "    def touch(self):\n",
    "        \"\"\"\n",
    "        Mark this update as complete.\n",
    "\n",
    "        The document id would be sufficient but,\n",
    "        for documentation,\n",
    "        we index the parameters `update_id`, `target_index`, `target_doc_type` and `date` as well.\n",
    "        \"\"\"\n",
    "        self.create_marker_index()\n",
    "        \"\"\"###modif \n",
    "        self.es.index(index=self.marker_index, doc_type=self.marker_doc_type,\n",
    "                      id=self.marker_index_document_id(), body={\n",
    "                          'update_id': self.update_id,\n",
    "                          'target_index': self.index,\n",
    "                          'target_doc_type': self.doc_type,\n",
    "                          'date': datetime.datetime.now()})\n",
    "        \"\"\"\n",
    "        self.es.index(index=self.marker_index, doc_type=self.marker_doc_type,\n",
    "                      id=self.marker_index_document_id(), body={\n",
    "                          'update_id': self.update_id,\n",
    "                          'target_index': self.index,\n",
    "                           'date': datetime.datetime.now()})\n",
    "        \n",
    "        self.es.indices.flush(index=self.marker_index)\n",
    "        self.ensure_hist_size()\n",
    "    def exists(self):\n",
    "        \"\"\"\n",
    "        Test, if this task has been run.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            #modif \n",
    "            #self.es.get(index=self.marker_index, doc_type=self.marker_doc_type,\n",
    "            #            id=self.marker_index_document_id())\n",
    "            self.es.get(index=self.marker_index,\n",
    "                        id=self.marker_index_document_id())\n",
    "            return True\n",
    "        except elasticsearch.NotFoundError:\n",
    "            logger.debug('Marker document not found.')\n",
    "        except elasticsearch.ElasticsearchException as err:\n",
    "            logger.warn(err)\n",
    "        return False\n",
    "    def create_marker_index(self):\n",
    "        \"\"\"\n",
    "        Create the index that will keep track of the tasks if necessary.\n",
    "        \"\"\"\n",
    "        if not self.es.indices.exists(index=self.marker_index):\n",
    "            self.es.indices.create(index=self.marker_index)\n",
    "    def ensure_hist_size(self):\n",
    "        \"\"\"\n",
    "        Shrink the history of updates for\n",
    "        a `index/doc_type` combination down to `self.marker_index_hist_size`.\n",
    "        \"\"\"\n",
    "        if self.marker_index_hist_size == 0:\n",
    "            return\n",
    "        ### modif\n",
    "        result = self.es.search(index=self.marker_index,\n",
    "                                #doc_type=self.marker_doc_type,\n",
    "                                body={'query': {\n",
    "                                    'term': {'target_index': self.index}}},\n",
    "                                sort=('date:desc',))\n",
    "\n",
    "        for i, hit in enumerate(result.get('hits').get('hits'), start=1):\n",
    "            if i > self.marker_index_hist_size:\n",
    "                marker_document_id = hit.get('_id')\n",
    "                ###modif\n",
    "                self.es.delete(id=marker_document_id, index=self.marker_index,\n",
    "                               #doc_type=self.marker_doc_type,\n",
    "                              )\n",
    "        self.es.indices.flush(index=self.marker_index)\n",
    "\n",
    "\n",
    "class CopyToIndex(luigi.Task):\n",
    "    \"\"\"\n",
    "    Template task for inserting a data set into Elasticsearch.\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    1. Subclass and override the required `index` attribute.\n",
    "\n",
    "    2. Implement a custom `docs` method, that returns an iterable over the documents.\n",
    "       A document can be a JSON string,\n",
    "       e.g. from a newline-delimited JSON (ldj) file (default implementation)\n",
    "       or some dictionary.\n",
    "\n",
    "    Optional attributes:\n",
    "\n",
    "    * doc_type (default),\n",
    "    * host (localhost),\n",
    "    * port (9200),\n",
    "    * settings ({'settings': {}})\n",
    "    * mapping (None),\n",
    "    * chunk_size (2000),\n",
    "    * raise_on_error (True),\n",
    "    * purge_existing_index (False),\n",
    "    * marker_index_hist_size (0)\n",
    "\n",
    "    If settings are defined, they are only applied at index creation time.\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def host(self):\n",
    "        \"\"\"\n",
    "        ES hostname.\n",
    "        \"\"\"\n",
    "        return 'localhost'\n",
    "\n",
    "    @property\n",
    "    def port(self):\n",
    "        \"\"\"\n",
    "        ES port.\n",
    "        \"\"\"\n",
    "        return 9200\n",
    "\n",
    "    @property\n",
    "    def http_auth(self):\n",
    "        \"\"\"\n",
    "        ES optional http auth information as either â:â separated string or a tuple,\n",
    "        e.g. `('user', 'pass')` or `\"user:pass\"`.\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def index(self):\n",
    "        \"\"\"\n",
    "        The target index.\n",
    "\n",
    "        May exist or not.\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def doc_type(self):\n",
    "        \"\"\"\n",
    "        The target doc_type.\n",
    "        \"\"\"\n",
    "        return 'default'\n",
    "\n",
    "    @property\n",
    "    def mapping(self):\n",
    "        \"\"\"\n",
    "        Dictionary with custom mapping or `None`.\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def settings(self):\n",
    "        \"\"\"\n",
    "        Settings to be used at index creation time.\n",
    "        \"\"\"\n",
    "        return {'settings': {}}\n",
    "\n",
    "    @property\n",
    "    def chunk_size(self):\n",
    "        \"\"\"\n",
    "        Single API call for this number of docs.\n",
    "        \"\"\"\n",
    "        return 2000\n",
    "\n",
    "    @property\n",
    "    def raise_on_error(self):\n",
    "        \"\"\"\n",
    "        Whether to fail fast.\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def purge_existing_index(self):\n",
    "        \"\"\"\n",
    "        Whether to delete the `index` completely before any indexing.\n",
    "        \"\"\"\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def marker_index_hist_size(self):\n",
    "        \"\"\"\n",
    "        Number of event log entries in the marker index. 0: unlimited.\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def timeout(self):\n",
    "        \"\"\"\n",
    "        Timeout.\n",
    "        \"\"\"\n",
    "        return 10\n",
    "\n",
    "    @property\n",
    "    def extra_elasticsearch_args(self):\n",
    "        \"\"\"\n",
    "        Extra arguments to pass to the Elasticsearch constructor\n",
    "        \"\"\"\n",
    "        return {}\n",
    "\n",
    "    def docs(self):\n",
    "        \"\"\"\n",
    "        Return the documents to be indexed.\n",
    "\n",
    "        Beside the user defined fields, the document may contain an `_index`, `_type` and `_id`.\n",
    "        \"\"\"\n",
    "        with self.input().open('r') as fobj:\n",
    "            for line in fobj:\n",
    "                yield line\n",
    "\n",
    "# everything below will rarely have to be overridden\n",
    "\n",
    "    def _docs(self):\n",
    "        \"\"\"\n",
    "        Since `self.docs` may yield documents that do not explicitly contain `_index` or `_type`,\n",
    "        add those attributes here, if necessary.\n",
    "        \"\"\"\n",
    "        iterdocs = iter(self.docs())\n",
    "        first = next(iterdocs)\n",
    "        needs_parsing = False\n",
    "        if isinstance(first, str):\n",
    "            needs_parsing = True\n",
    "        elif isinstance(first, dict):\n",
    "            pass\n",
    "        else:\n",
    "            raise RuntimeError('Document must be either JSON strings or dict.')\n",
    "        for doc in itertools.chain([first], iterdocs):\n",
    "            if needs_parsing:\n",
    "                doc = json.loads(doc)\n",
    "            if '_index' not in doc:\n",
    "                doc['_index'] = self.index\n",
    "            ### modif\n",
    "            #if '_type' not in doc:\n",
    "            #    doc['_type'] = self.doc_type\n",
    "            \n",
    "            yield doc\n",
    "\n",
    "    def _init_connection(self):\n",
    "        return elasticsearch.Elasticsearch(\n",
    "            connection_class=Urllib3HttpConnection,\n",
    "            host=self.host,\n",
    "            port=self.port,\n",
    "            http_auth=self.http_auth,\n",
    "            timeout=self.timeout,\n",
    "            **self.extra_elasticsearch_args\n",
    "        )\n",
    "\n",
    "    def create_index(self):\n",
    "        \"\"\"\n",
    "        Override to provide code for creating the target index.\n",
    "\n",
    "        By default it will be created without any special settings or mappings.\n",
    "        \"\"\"\n",
    "        es = self._init_connection()\n",
    "        if not es.indices.exists(index=self.index):\n",
    "            es.indices.create(index=self.index, body=self.settings)\n",
    "\n",
    "    def delete_index(self):\n",
    "        \"\"\"\n",
    "        Delete the index, if it exists.\n",
    "        \"\"\"\n",
    "        es = self._init_connection()\n",
    "        if es.indices.exists(index=self.index):\n",
    "            es.indices.delete(index=self.index)\n",
    "\n",
    "    def update_id(self):\n",
    "        \"\"\"\n",
    "        This id will be a unique identifier for this indexing task.\n",
    "        \"\"\"\n",
    "        return self.task_id\n",
    "\n",
    "    def output(self):\n",
    "        \"\"\"\n",
    "        Returns a ElasticsearchTarget representing the inserted dataset.\n",
    "\n",
    "        Normally you don't override this.\n",
    "        \"\"\"\n",
    "        ### modif\n",
    "        return ElasticsearchTarget(\n",
    "            host=self.host,\n",
    "            port=self.port,\n",
    "            http_auth=self.http_auth,\n",
    "            index=self.index,\n",
    "            #doc_type=self.doc_type,\n",
    "            update_id=self.update_id(),\n",
    "            marker_index_hist_size=self.marker_index_hist_size,\n",
    "            timeout=self.timeout,\n",
    "            extra_elasticsearch_args=self.extra_elasticsearch_args\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run task, namely:\n",
    "\n",
    "        * purge existing index, if requested (`purge_existing_index`),\n",
    "        * create the index, if missing,\n",
    "        * apply mappings, if given,\n",
    "        * set refresh interval to -1 (disable) for performance reasons,\n",
    "        * bulk index in batches of size `chunk_size` (2000),\n",
    "        * set refresh interval to 1s,\n",
    "        * refresh Elasticsearch,\n",
    "        * create entry in marker index.\n",
    "        \"\"\"\n",
    "        if self.purge_existing_index:\n",
    "            self.delete_index()\n",
    "        self.create_index()\n",
    "        es = self._init_connection()\n",
    "        if self.mapping:\n",
    "            ####modif\n",
    "            es.indices.put_mapping(index=self.index,\n",
    "                                   #doc_type=self.doc_type,\n",
    "                                   body=self.mapping)\n",
    "        es.indices.put_settings({\"index\": {\"refresh_interval\": \"-1\"}},\n",
    "                                index=self.index)\n",
    "\n",
    "        bulk(es, self._docs(), chunk_size=self.chunk_size,\n",
    "             raise_on_error=self.raise_on_error)\n",
    "\n",
    "        es.indices.put_settings({\"index\": {\"refresh_interval\": \"1s\"}},\n",
    "                                index=self.index)\n",
    "        es.indices.refresh()\n",
    "        self.output().touch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Tests for Elasticsearch index (esindex) target and indexing.\n",
    "\"\"\"\n",
    "\n",
    "# pylint: disable=C0103,E1101,F0401\n",
    "from luigi.contrib.esindex import ElasticsearchTarget, CopyToIndex\n",
    "import collections\n",
    "import datetime\n",
    "import elasticsearch\n",
    "import luigi\n",
    "import unittest\n",
    "\n",
    "\n",
    "HOST = 'localhost'\n",
    "PORT = 9200\n",
    "INDEX = 'esindex_luigi_test'\n",
    "###modif\n",
    "#DOC_TYPE = 'esindex_test_type'\n",
    "MARKER_INDEX = 'esindex_luigi_test_index_updates'\n",
    "MARKER_DOC_TYPE = 'esindex_test_entry'\n",
    "\n",
    "\n",
    "def _create_test_index():\n",
    "    \"\"\" Create content index, if if does not exists. \"\"\"\n",
    "    es = elasticsearch.Elasticsearch([{'host': HOST, 'port': PORT}])\n",
    "    if not es.indices.exists(INDEX):\n",
    "        es.indices.create(INDEX)\n",
    "\n",
    "\n",
    "_create_test_index()\n",
    "###modif\n",
    "target = ElasticsearchTarget(HOST, PORT, INDEX,\n",
    "                             #DOC_TYPE,\n",
    "                             'update_id')\n",
    "target.marker_index = MARKER_INDEX\n",
    "###modif\n",
    "#target.marker_doc_type = MARKER_DOC_TYPE\n",
    "\n",
    "\n",
    "class ElasticsearchTargetTest(unittest.TestCase):\n",
    "    \"\"\" Test touch and exists. \"\"\"\n",
    "    def test_touch_and_exists(self):\n",
    "        \"\"\" Basic test. \"\"\"\n",
    "        delete()\n",
    "        self.assertFalse(target.exists(),\n",
    "                         'Target should not exist before touching it')\n",
    "        target.touch()\n",
    "        self.assertTrue(target.exists(),\n",
    "                        'Target should exist after touching it')\n",
    "        delete()\n",
    "\n",
    "\n",
    "def delete():\n",
    "    \"\"\" Delete marker_index, if it exists. \"\"\"\n",
    "    es = elasticsearch.Elasticsearch([{'host': HOST, 'port': PORT}])\n",
    "    if es.indices.exists(MARKER_INDEX):\n",
    "        es.indices.delete(MARKER_INDEX)\n",
    "    es.indices.refresh()\n",
    "\n",
    "\n",
    "class CopyToTestIndex(CopyToIndex):\n",
    "    \"\"\" Override the default `marker_index` table with a test name. \"\"\"\n",
    "    host = HOST\n",
    "    port = PORT\n",
    "    index = INDEX\n",
    "    ###modif\n",
    "    #doc_type = DOC_TYPE\n",
    "    marker_index_hist_size = 0\n",
    "\n",
    "    def output(self):\n",
    "        \"\"\" Use a test target with an own marker_index. \"\"\"\n",
    "        ###modif\n",
    "        target = ElasticsearchTarget(\n",
    "            host=self.host,\n",
    "            port=self.port,\n",
    "            index=self.index,\n",
    "            #doc_type=self.doc_type,\n",
    "            update_id=self.update_id(),\n",
    "            marker_index_hist_size=self.marker_index_hist_size\n",
    "         )\n",
    "        target.marker_index = MARKER_INDEX\n",
    "        ###modif\n",
    "        #target.marker_doc_type = MARKER_DOC_TYPE\n",
    "        return target\n",
    "\n",
    "\n",
    "class IndexingTask1(CopyToTestIndex):\n",
    "    \"\"\" Test the redundant version, where `_index` and `_type` are\n",
    "    given in the `docs` as well. A more DRY example is `IndexingTask2`. \"\"\"\n",
    "    def docs(self):\n",
    "        \"\"\" Return a list with a single doc. \"\"\"\n",
    "        return [{'_id': 123, '_index': self.index, '_type': self.doc_type,\n",
    "                'name': 'sample', 'date': 'today'}]\n",
    "\n",
    "\n",
    "class IndexingTask2(CopyToTestIndex):\n",
    "    \"\"\" Just another task. \"\"\"\n",
    "    def docs(self):\n",
    "        \"\"\" Return a list with a single doc. \"\"\"\n",
    "        ###modif\n",
    "        return [{'_id': 234, '_index': self.index,\n",
    "                 #'_type': self.doc_type,\n",
    "                 'name': 'another', 'date': 'today'}]\n",
    "\n",
    "\n",
    "class IndexingTask3(CopyToTestIndex):\n",
    "    \"\"\" This task will request an empty index to start with. \"\"\"\n",
    "    purge_existing_index = True\n",
    "    def docs(self):\n",
    "        \"\"\" Return a list with a single doc. \"\"\"\n",
    "        ###modif\n",
    "        return [{'_id': 234, '_index': self.index,\n",
    "                 #'_type': self.doc_type,\n",
    "                 'name': 'yet another', 'date': 'today'}]\n",
    "\n",
    "\n",
    "def _cleanup():\n",
    "    \"\"\" Delete both the test marker index and the content index. \"\"\"\n",
    "    es = elasticsearch.Elasticsearch([{'host': HOST, 'port': PORT}])\n",
    "    if es.indices.exists(MARKER_INDEX):\n",
    "        es.indices.delete(MARKER_INDEX)\n",
    "    if es.indices.exists(INDEX):\n",
    "        es.indices.delete(INDEX)\n",
    "\n",
    "\n",
    "class CopyToIndexTest(unittest.TestCase):\n",
    "    \"\"\" Test indexing tasks. \"\"\"\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\" Cleanup before each test. \"\"\"\n",
    "        _cleanup()\n",
    "\n",
    "    def tearDown(self):\n",
    "        \"\"\" Remove residues after each test. \"\"\"\n",
    "        _cleanup()\n",
    "\n",
    "    def test_copy_to_index(self):\n",
    "        \"\"\" Test a single document upload. \"\"\"\n",
    "        task = IndexingTask1()\n",
    "        es = elasticsearch.Elasticsearch([{'host': HOST, 'port': PORT}])\n",
    "        self.assertFalse(es.indices.exists(task.index))\n",
    "        self.assertFalse(task.complete())\n",
    "        luigi.build([task], local_scheduler=True)\n",
    "        self.assertTrue(es.indices.exists(task.index))\n",
    "        self.assertTrue(task.complete())\n",
    "        self.assertEquals(1, es.count(index=task.index).get('count'))\n",
    "        self.assertEquals({u'date': u'today', u'name': u'sample'},\n",
    "                          es.get_source(index=task.index,\n",
    "                                        ###modif\n",
    "                                        doc_type=task.doc_type,\n",
    "                                        id=123))\n",
    "\n",
    "    def test_copy_to_index_incrementally(self):\n",
    "        \"\"\" Test two tasks that upload docs into the same index. \"\"\"\n",
    "        task1 = IndexingTask1()\n",
    "        task2 = IndexingTask2()\n",
    "        es = elasticsearch.Elasticsearch([{'host': HOST, 'port': PORT}])\n",
    "        self.assertFalse(es.indices.exists(task1.index))\n",
    "        self.assertFalse(es.indices.exists(task2.index))\n",
    "        self.assertFalse(task1.complete())\n",
    "        self.assertFalse(task2.complete())\n",
    "        luigi.build([task1, task2], local_scheduler=True)\n",
    "        self.assertTrue(es.indices.exists(task1.index))\n",
    "        self.assertTrue(es.indices.exists(task2.index))\n",
    "        self.assertTrue(task1.complete())\n",
    "        self.assertTrue(task2.complete())\n",
    "        self.assertEquals(2, es.count(index=task1.index).get('count'))\n",
    "        self.assertEquals(2, es.count(index=task2.index).get('count'))\n",
    "\n",
    "        self.assertEquals({u'date': u'today', u'name': u'sample'},\n",
    "                          es.get_source(index=task1.index,\n",
    "                                        ###modif \n",
    "                                        #doc_type=task1.doc_type,\n",
    "                                        id=123))\n",
    "\n",
    "        self.assertEquals({u'date': u'today', u'name': u'another'},\n",
    "                          es.get_source(index=task2.index,\n",
    "                                        ###modif\n",
    "                                        #doc_type=task2.doc_type,\n",
    "                                        id=234))\n",
    "\n",
    "    def test_copy_to_index_purge_existing(self):\n",
    "        \"\"\" Test purge_existing_index purges index. \"\"\"\n",
    "        task1 = IndexingTask1()\n",
    "        task2 = IndexingTask2()\n",
    "        task3 = IndexingTask3()\n",
    "        luigi.build([task1, task2], local_scheduler=True)\n",
    "        luigi.build([task3], local_scheduler=True)\n",
    "        es = elasticsearch.Elasticsearch([{'host': HOST, 'port': PORT}])\n",
    "        self.assertTrue(es.indices.exists(task3.index))\n",
    "        self.assertTrue(task3.complete())\n",
    "        self.assertEquals(1, es.count(index=task3.index).get('count'))\n",
    "\n",
    "        self.assertEquals({u'date': u'today', u'name': u'yet another'},\n",
    "                          es.get_source(index=task3.index,\n",
    "                                        ###modif\n",
    "                                        #doc_type=task3.doc_type,\n",
    "                                        id=234))\n",
    "\n",
    "\n",
    "class MarkerIndexTest(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\" Cleanup before each test. \"\"\"\n",
    "        _cleanup()\n",
    "\n",
    "    def tearDown(self):\n",
    "        \"\"\" Remove residues after each test. \"\"\"\n",
    "        _cleanup()\n",
    "\n",
    "    def test_update_marker(self):\n",
    "        es = elasticsearch.Elasticsearch()\n",
    "        with self.assertRaises(elasticsearch.NotFoundError):\n",
    "            ###modif\n",
    "            result = es.count(index=MARKER_INDEX,\n",
    "                              #doc_type=MARKER_DOC_TYPE,\n",
    "                              body={'query': {'match_all': {}}})\n",
    "\n",
    "        task1 = IndexingTask1()\n",
    "        luigi.build([task1], local_scheduler=True)\n",
    "\n",
    "        result = es.count(index=MARKER_INDEX,\n",
    "                          ###modif\n",
    "                          #doc_type=MARKER_DOC_TYPE,\n",
    "                           body={'query': {'match_all': {}}})\n",
    "        self.assertEquals(1, result.get('count'))\n",
    "\n",
    "        result = es.search(index=MARKER_INDEX,\n",
    "                           ###modif\n",
    "                           #doc_type=MARKER_DOC_TYPE,\n",
    "                           body={'query': {'match_all': {}}})\n",
    "        marker_doc = result.get('hits').get('hits')[0].get('_source')\n",
    "        self.assertEquals('IndexingTask1()', marker_doc.get('update_id'))\n",
    "        self.assertEquals(INDEX, marker_doc.get('target_index'))\n",
    "        ###modif\n",
    "        #self.assertEquals(DOC_TYPE, marker_doc.get('target_doc_type'))\n",
    "        self.assertTrue('date' in marker_doc)\n",
    "\n",
    "        task2 = IndexingTask2()\n",
    "        luigi.build([task2], local_scheduler=True)\n",
    "\n",
    "        result = es.count(index=MARKER_INDEX,\n",
    "                          ###modif\n",
    "                          #doc_type=MARKER_DOC_TYPE,\n",
    "                           body={'query': {'match_all': {}}})\n",
    "        self.assertEquals(2, result.get('count'))\n",
    "\n",
    "\n",
    "        result = es.search(index=MARKER_INDEX,\n",
    "                           ###modif\n",
    "                           #doc_type=MARKER_DOC_TYPE,\n",
    "                           body={'query': {'match_all': {}}})\n",
    "        hits = result.get('hits').get('hits')\n",
    "        Entry = collections.namedtuple('Entry', ['date', 'update_id'])\n",
    "        dates_update_id = []\n",
    "        for hit in hits:\n",
    "            source = hit.get('_source')\n",
    "            update_id = source.get('update_id')\n",
    "            date = source.get('date')\n",
    "            dates_update_id.append(Entry(date, update_id))\n",
    "\n",
    "        it = iter(sorted(dates_update_id))\n",
    "        first = it.next()\n",
    "        second = it.next()\n",
    "        self.assertTrue(first.date < second.date)\n",
    "        self.assertEquals(first.update_id, 'IndexingTask1()')\n",
    "        self.assertEquals(second.update_id, 'IndexingTask2()')\n",
    "\n",
    "\n",
    "class IndexingTask4(CopyToTestIndex):\n",
    "    \"\"\" Just another task. \"\"\"\n",
    "    date = luigi.DateParameter(default=datetime.date(1970, 1, 1))\n",
    "    marker_index_hist_size = 1\n",
    "\n",
    "    def docs(self):\n",
    "        \"\"\" Return a list with a single doc. \"\"\"\n",
    "        return [{'_id': 234, '_index': self.index,\n",
    "                 ###modif\n",
    "                 #'_type': self.doc_type,\n",
    "                 'name': 'another', 'date': 'today'}]\n",
    "\n",
    "class IndexHistSizeTest(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\" Cleanup before each test. \"\"\"\n",
    "        _cleanup()\n",
    "\n",
    "    def tearDown(self):\n",
    "        \"\"\" Remove residues after each test. \"\"\"\n",
    "        _cleanup()\n",
    "\n",
    "    def test_limited_history(self):\n",
    "\n",
    "        task4_1 = IndexingTask4(date=datetime.date(2000, 1, 1))\n",
    "        luigi.build([task4_1], local_scheduler=True)\n",
    "\n",
    "        task4_2 = IndexingTask4(date=datetime.date(2001, 1, 1))\n",
    "        luigi.build([task4_2], local_scheduler=True)\n",
    "\n",
    "        task4_3 = IndexingTask4(date=datetime.date(2002, 1, 1))\n",
    "        luigi.build([task4_3], local_scheduler=True)\n",
    "\n",
    "        es = elasticsearch.Elasticsearch()\n",
    "\n",
    "        result = es.count(index=MARKER_INDEX,\n",
    "                          ###modif\n",
    "                          #doc_type=MARKER_DOC_TYPE,\n",
    "                          body={'query': {'match_all': {}}})\n",
    "        self.assertEquals(1, result.get('count'))\n",
    "        marker_index_document_id = task4_3.output().marker_index_document_id()\n",
    "        result = es.get(id=marker_index_document_id, index=MARKER_INDEX,\n",
    "                        ###modif\n",
    "                        #doc_type=MARKER_DOC_TYPE,\n",
    "                       )\n",
    "        self.assertEquals('IndexingTask4(date=2002-01-01)',\n",
    "                          result.get('_source').get('update_id'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import luigi\n",
    "import os,sys\n",
    "from luigi.contrib.esindex  import CopyToIndex,ElasticsearchTarget\n",
    "\n",
    "class ExampleIndex(CopyToIndex):\n",
    "    host = 'localhost'\n",
    "    port = 9200\n",
    "    index = 'example'\n",
    "    doc_type = 'default'\n",
    "    purge_existing_index = True\n",
    "    marker_index_hist_size = 1\n",
    "\n",
    "    def docs(self):\n",
    "        return [{'_id': 1, 'title': 'An example document.'}]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    task = ExampleIndex()\n",
    "    luigi.build([task], workers = 5,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Copyright 2012-2015 Spotify AB\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "import luigi\n",
    "from luigi.contrib.esindex import CopyToIndex\n",
    "\n",
    "\n",
    "class FakeDocuments(luigi.Task):\n",
    "    \"\"\"\n",
    "    Generates a local file containing 5 elements of data in JSON format.\n",
    "    \"\"\"\n",
    "\n",
    "    #: the date parameter.\n",
    "    date = luigi.DateParameter(default=datetime.date.today())\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Writes data in JSON format into the task's output target.\n",
    "\n",
    "        The data objects have the following attributes:\n",
    "\n",
    "        * `_id` is the default Elasticsearch id field,\n",
    "        * `text`: the text,\n",
    "        * `date`: the day when the data was created.\n",
    "\n",
    "        \"\"\"\n",
    "        today = datetime.date.today()\n",
    "        with self.output().open('w') as output:\n",
    "            for i in range(5):\n",
    "                output.write(json.dumps({'_id': i, 'text': 'Hi %s' % i,\n",
    "                                         'date': str(today)}))\n",
    "                output.write('\\n')\n",
    "\n",
    "    def output(self):\n",
    "        \"\"\"\n",
    "        Returns the target output for this task.\n",
    "        In this case, a successful execution of this task will create a file on the local filesystem.\n",
    "\n",
    "        :return: the target output for this task.\n",
    "        :rtype: object (:py:class:`luigi.target.Target`)\n",
    "        \"\"\"\n",
    "        return luigi.LocalTarget(path='/tmp/_docs-%s.ldj' % self.date)\n",
    "\n",
    "\n",
    "class IndexDocuments(CopyToIndex):\n",
    "    \"\"\"\n",
    "    This task loads JSON data contained in a :py:class:`luigi.target.Target` into an ElasticSearch index.\n",
    "\n",
    "    This task's input will the target returned by :py:meth:`~.FakeDocuments.output`.\n",
    "\n",
    "    This class uses :py:meth:`luigi.contrib.esindex.CopyToIndex.run`.\n",
    "\n",
    "    After running this task you can run:\n",
    "\n",
    "    .. code-block:: console\n",
    "\n",
    "        $ curl \"localhost:9200/example_index/_search?pretty\"\n",
    "\n",
    "    to see the indexed documents.\n",
    "\n",
    "    To see the update log, run\n",
    "\n",
    "    .. code-block:: console\n",
    "\n",
    "        $ curl \"localhost:9200/update_log/_search?q=target_index:example_index&pretty\"\n",
    "\n",
    "    To cleanup both indexes run:\n",
    "\n",
    "    .. code-block:: console\n",
    "\n",
    "        $ curl -XDELETE \"localhost:9200/example_index\"\n",
    "        $ curl -XDELETE \"localhost:9200/update_log/_query?q=target_index:example_index\"\n",
    "\n",
    "    \"\"\"\n",
    "    #: date task parameter (default = today)\n",
    "    date = luigi.DateParameter(default=datetime.date.today())\n",
    "\n",
    "    #: the name of the index in ElasticSearch to be updated.\n",
    "    index = 'example_index'\n",
    "    #: the name of the document type.\n",
    "    doc_type = 'greetings'\n",
    "    #: the host running the ElasticSearch service.\n",
    "    host = 'localhost'\n",
    "    #: the port used by the ElasticSearch service.\n",
    "    port = 9200\n",
    "\n",
    "    def requires(self):\n",
    "        \"\"\"\n",
    "        This task's dependencies:\n",
    "\n",
    "        * :py:class:`~.FakeDocuments`\n",
    "\n",
    "        :return: object (:py:class:`luigi.task.Task`)\n",
    "        \"\"\"\n",
    "        return FakeDocuments()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    luigi.run(['IndexDocuments', '--local-scheduler'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Patrick",
   "language": "python",
   "name": "patrick"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
