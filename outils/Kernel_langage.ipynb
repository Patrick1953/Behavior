{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%save Kernel_langage.py 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Class Kernel qui contient\n",
    "\n",
    "- pour elasticsearh :\n",
    "            -parametres (host et port)\n",
    "            -fonction bulk (actions, index)\n",
    "            -fonction creation index (index) return r (resultat)\n",
    "            -fonction delete index (index)\n",
    "            -count (index) renvoie le nombre d'Ã©lement\n",
    "\n",
    "- creation de l'index regroupement (cas Lexique)\n",
    "\n",
    "            - index (regroupement) \n",
    "            - liste_type\n",
    "            - debut nom des fichiers text des regroupents du Lexique par type\n",
    "                  (debut_fichier_regroupement)\n",
    "            - liste des types du Lexique\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "Nonesudo apt autoremove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%save Kernel_langage.py 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "class Kernel () :\n",
    "    def __init__ (self, host = \"localhost\", \n",
    "                 port = 9200 ,\n",
    "                 trace = False,\n",
    "                ) :\n",
    "        \n",
    "        self.trace = trace\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        \n",
    "        self.elastic = Elasticsearch([{\"host\":host , \"port\":port }]  )\n",
    "        \n",
    "        self.liste_type = ['NOM', 'AUX', 'VER', 'ADV', 'PRE', 'ADJ', 'ONO', 'CON',\n",
    "                  'ART:def', 'ADJ:ind', 'PRO:ind', 'PRO:int', 'PRO:rel',\n",
    "                  'ADJ:num', 'PRO:dem', 'ADJ:dem', 'PRO:per', 'ART:ind',\n",
    "                  'LIA', 'PRO:pos', 'ADJ:pos', '', 'ADJ:int']\n",
    "        \n",
    "        self.index_regroupement = \"regroupement\"\n",
    "        self.settings_regroupement = { \"settings\" : {\n",
    "                                      'index.mapping.total_fields.limit':100000,\n",
    "                                     }\n",
    "        }\n",
    "        \n",
    "        self.dico_settings = {'regroupement' : self.settings_regroupement ,\n",
    "                             }\n",
    "                              \n",
    "                              \n",
    "        \n",
    "        self.structure_du_lexique = [\"1_ortho\" , \n",
    "                                     \"3_lemme\" , \n",
    "                                     \"4_cgram\" , \n",
    "                                     \"5_genre\" , \n",
    "                                     \"6_nombre\" , \n",
    "                                     \"7_freqlemfilms2\" , \n",
    "                                     \"8_freqlemlivres\" , \n",
    "                                     \"9_freqfilms2\" , \n",
    "                                     \"10_freqlivres\" , \n",
    "                                     \"11_infover\" , \n",
    "                                     \"12_nbhomogr\" , \n",
    "                                     \"13_nbhomoph\" , \n",
    "                                     \"14_islem\" , \n",
    "                                     \"15_nblettres\" , \n",
    "                                     \"19_voisorth\" , \n",
    "                                     \"21_puorth\" , \n",
    "                                     \"22_puphon\" , \n",
    "                                     \"29_cgramortho\" , \n",
    "                                     \"30_deflem\" , \n",
    "                                     \"31_defobs\" , \n",
    "                                     \"32_old20\" , \n",
    "                                     \"33_pld20\" , \n",
    "                                     \"35_nbmorph\" , \n",
    "                                    ]\n",
    "        \n",
    "\n",
    "        \n",
    "        self.debut_fichier_regroupement = \"../../../data/Lexique383_group\"\n",
    "        \n",
    "        \n",
    "        \n",
    "    def bulk (self,actions, index ):\n",
    "        \n",
    "        rr = []\n",
    "        for action in actions :\n",
    "            action [\"_index\"] = index\n",
    "            rr.append(action)\n",
    "            continue\n",
    "        \n",
    "        # make the bulk call using 'actions' and get a response\"\n",
    "        response = helpers.bulk(self.elastic, rr,\n",
    "                                stats_only= True, \n",
    "                                request_timeout = 360,\n",
    "                                raise_on_exception = False,\n",
    "                               )\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def count (self, index) :\n",
    "        self.elastic.indices.refresh(index)\n",
    "        r = self.elastic.cat.count(index, params={\"format\": \"json\"})\n",
    "        #print ('count =',r)\n",
    "        return int (r [0] ['count'])\n",
    "    \n",
    "    def creation_index (self, index) :\n",
    "\n",
    "        try:\n",
    "            self.elastic.indices.delete (index)\n",
    "        except:\n",
    "            pass\n",
    "        try :\n",
    "            settings = self.dico_settings [index]\n",
    "        except:\n",
    "            settings = {}\n",
    "                              \n",
    "        r = self.elastic.indices.create(index, body = settings)\n",
    "        if self.trace :\n",
    "            print (\"creation de l'index =\", r, )\n",
    "        return r['acknowledged']\n",
    "\n",
    "    def delete_index (self, index) :\n",
    "        try:\n",
    "            self.elastic.indices.delete (index)\n",
    "        except:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def search_mot (self, index, mot, zone) :\n",
    "        self.elastic.indices.refresh (index)\n",
    "        query = {'query' : {'match' : {zone : mot}}}\n",
    "        #print (\"query =\", query )\n",
    "        res= self.elastic.search (index= index, body = query )\n",
    "        #res = self.elastic.search(index=index , body={\"query\": {\"match_all\": {}}})\n",
    "        #print ([hit['_source'] for hit in res ['hits'] ['hits']])\n",
    "        return [hit['_source'] for hit in res ['hits'] ['hits']]\n",
    "    \n",
    "    def close (self,) :\n",
    "        self.elastic.close()\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creation kernel = OK\n",
      "liste_type = OK\n",
      "regroupement = OK\n",
      "creation_index = OK\n",
      "Kernel = OK\n",
      "creation index OK\n",
      "execution du bulk = OK pour 2 document (2, 0)\n",
      "search sur 2 docs OK\n",
      "on demarre big bulk :\n",
      "(500, 0)  pour nombre de document = 500\n",
      "(500, 0)  pour nombre de document = 1500\n",
      "(500, 0)  pour nombre de document = 2500\n",
      "(500, 0)  pour nombre de document = 3500\n",
      "(500, 0)  pour nombre de document = 4500\n",
      "(500, 0)  pour nombre de document = 5500\n",
      "(500, 0)  pour nombre de document = 6500\n",
      "(500, 0)  pour nombre de document = 7500\n",
      "(500, 0)  pour nombre de document = 8500\n",
      "(500, 0)  pour nombre de document = 9500\n",
      "execution du bulk = OK pour total de 10000 documents\n",
      "duree = 2.156339168548584\n",
      "count = OK\n",
      "duree = 0.02091217041015625\n",
      "search sur  10002  docs OK\n",
      "delete_index = OK\n",
      "Fin TEST\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "#from Kernel_langage import Kernel\n",
    "import time\n",
    "\n",
    "kernel = Kernel ()\n",
    "print (\"creation kernel = OK\")\n",
    "\n",
    "liste = kernel.liste_type\n",
    "if liste == ['NOM', 'AUX', 'VER', 'ADV', 'PRE', 'ADJ', 'ONO', 'CON', 'ART:def',\n",
    "             'ADJ:ind', 'PRO:ind', 'PRO:int', 'PRO:rel', 'ADJ:num', 'PRO:dem',\n",
    "             'ADJ:dem', 'PRO:per', 'ART:ind', 'LIA', 'PRO:pos', 'ADJ:pos', '',\n",
    "             'ADJ:int'] :\n",
    "    print ( \"liste_type = OK\")\n",
    "else :\n",
    "    print ( \"liste_type = KO\")\n",
    "    \n",
    "\n",
    "index = kernel.index_regroupement\n",
    "if index == \"regroupement\" :\n",
    "    print ( \"regroupement = OK\")\n",
    "else :\n",
    "    print ( \"regroupement = KO\")\n",
    "    \n",
    "\n",
    "r = kernel.creation_index (index,)\n",
    "  \n",
    "if r :\n",
    "    print ( \"creation_index = OK\")\n",
    "else:\n",
    "    print ( \"creation_index = KO\")\n",
    "    \n",
    "nameFile =  kernel.debut_fichier_regroupement\n",
    "if nameFile == \"../../../data/Lexique383_group\" :\n",
    "    print (\"Kernel = OK\")\n",
    "else :\n",
    "    print (\"Kernel  = KO\")\n",
    "    \n",
    "index = \"test\"\n",
    "r = kernel.creation_index (index)\n",
    "if r :\n",
    "    print ('creation index OK')\n",
    "else :\n",
    "    print ('creation index KO #############')\n",
    "    \n",
    "    \n",
    "actions = [ {'test' : \"test1\",\n",
    "            \"test_mot\" : \"test1\" },\n",
    "           \n",
    "            {\"test\": \"test2\",\n",
    "            \"test_mot\" : \"test2 test3\" },\n",
    "            ]\n",
    "\n",
    "r =kernel.bulk (actions, index)\n",
    "print (\"execution du bulk = OK pour 2 document\", r)\n",
    "\n",
    "mot = \"test2\"\n",
    "zone = \"test_mot\"\n",
    "\n",
    "res  = kernel.search_mot (index, mot, zone)\n",
    "\n",
    "if len(res) == 1 :\n",
    "    print (\"search sur 2 docs OK\")\n",
    "else :\n",
    "    print (\"search sur 2 docs KO ##############\")\n",
    "\n",
    "#print (kernel.count (index))\n",
    "print ('on demarre big bulk :')\n",
    "temps_depart = time.time()\n",
    "nombre = 500\n",
    "nombre_iteration = 20\n",
    "for k in range(0,nombre_iteration) :\n",
    "    actions = []\n",
    "    for i in range(0, nombre ) :\n",
    "        li = ''\n",
    "        for j in range(0,20 ) :\n",
    "            li += str (k)+'test' + str((100*i)+j)+ ' '\n",
    "            continue\n",
    "        li = li [ : -1]\n",
    "        doc_unitaire  = {'test' : 'test'+str(i),\n",
    "                          \"test_mot\" : li,\n",
    "                         }\n",
    "        \n",
    "        actions.append(doc_unitaire)\n",
    "        continue\n",
    "    \n",
    "    r =kernel.bulk (actions, index)\n",
    "    if (nombre*k)%1000 == 0 : \n",
    "        print (r, \" pour nombre de document =\", nombre*(k+1) )\n",
    "        #print (\" li =\", li)\n",
    "    continue\n",
    "    \n",
    "\n",
    "    \n",
    "print (\"execution du bulk = OK pour total de \" + str(nombre_iteration*nombre) + ' documents')\n",
    "print ( \"duree =\", time.time() -temps_depart)\n",
    "\n",
    "\n",
    "nombre_voulu = (nombre_iteration*nombre) + 2\n",
    "\n",
    "nombre = kernel.count (index)\n",
    "\n",
    "if nombre == nombre_voulu :\n",
    "    print ('count = OK')\n",
    "else :\n",
    "    print (\"nombre =\", nombre)\n",
    "    print (\"nombre_voulu =\", nombre_voulu)\n",
    "    print ('count =KO #################')\n",
    "    \n",
    "mot = \"0test101\"\n",
    "zone = \"test_mot\"\n",
    "temps_depart = time.time()\n",
    "res  = kernel.search_mot(index, mot, zone)\n",
    "print (\"duree =\", time.time() - temps_depart)\n",
    "if len(res) == 1 :\n",
    "    print (\"search sur \", nombre, \" docs OK\")\n",
    "else :\n",
    "    print (\"search sur 2 docs KO ##############\")\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "r = kernel.delete_index (index)\n",
    "\n",
    "if r :\n",
    "    print ('delete_index = OK')\n",
    "else:\n",
    "    print('delete_index = KO #####################')\n",
    "    \n",
    "\n",
    "\n",
    "kernel.close()\n",
    "\n",
    "print ('Fin TEST')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10002"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Kernel_langage import Kernel\n",
    "\n",
    "\n",
    "kernel = Kernel ()\n",
    "\n",
    "index = kernel.index_regroupement\n",
    "\n",
    "zone = \"liste_mot_lie\"\n",
    "\n",
    "kernel.search_mot(index, \"avoir\", zone)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch()\n",
    "\n",
    "doc = {\n",
    "    'author': 'kimchy',\n",
    "    'text': 'Elasticsearch: cool. bonsai cool.',\n",
    "    \n",
    "}\n",
    "res = es.index(index=\"test-index\", id=1, body=doc)\n",
    "print(res['result'])\n",
    "\n",
    "res = es.get(index=\"test-index\", id=1)\n",
    "print(res['_source'])\n",
    "\n",
    "es.indices.refresh(index=\"test-index\")\n",
    "index = \"test-index\"\n",
    "res = es.search(index=index, body={\"query\":{\"match\":  {'text': 'bonsai'}}})\n",
    "print(\"Got %d Hits:\" % res['hits']['total']['value'])\n",
    "for hit in res['hits']['hits']:\n",
    "    print (hit[\"_source\"])\n",
    "    #print(\" %(author)s %(text)s\" % hit[\"_source\"])\n",
    "    \n",
    "es.indices.delete (\"test-index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
